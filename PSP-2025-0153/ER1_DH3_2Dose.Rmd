---
title: "ER1_empirical dose(DH3) with 2 DOSE levels"
author: "Xuefen Yin"
date: "2024-11-27"
output: html_document
---

# R setup
```{r setup, echo = F}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

rm(list=ls(all=TRUE))

library(tidyverse)
library(mrgsolve)
library(ggplot2)
library(survival)
library(survminer)
library(gridExtra)
library(zoo)
library(openxlsx)

new_path <- paste("C:/rtools44/usr/bin", "C:/rtools44/mingw64/bin", Sys.getenv("PATH"), sep=";")
Sys.setenv(PATH = new_path)

```

#1 Common functions(run one time)

##1.1 common function for plot
```{r}
theme_set(theme_bw())

custom_theme <- theme_classic() +
  theme(
    panel.grid.major = element_line(color = "gray70", linewidth = 0.5),  # Major grid lines
    panel.grid.minor = element_line(color = "gray90", linewidth = 0.25)  # Minor grid lines
  )

#1. logistic regression profile
logistic_regression_plot <- function(data, predictor, response) {
  # Ensure predictor is numeric for log scale
  data[[predictor]] <- as.numeric(data[[predictor]])
  
  plot <- data %>%
    ggplot(aes_string(x = predictor, y = response)) +
    geom_point(shape = "|") +
    geom_smooth(method = "glm", method.args = list(family = "binomial")) +
    scale_x_log10() +
    labs(x = predictor,
         y = "Probability of Event",
         title = paste("Logistic Regression -", predictor))
  
  return(plot)
}

##2. KM plot having 3 groups(EO2)
KM_plot_T <- function(km_object, data, title_prefix, plot_x_scale = 600,save_path) {
  plot <- ggsurvplot(
    km_object,
    data = data,
    surv.median.line = "hv",
    conf.int = TRUE,
    # pval = TRUE,
    # pval.method = TRUE,
    risk.table = TRUE,
    risk.table.y.text = FALSE,
    tables.theme = theme_cleantable(),
    tables.height = 0.25,
    palette = c("red", "limegreen", "blue2"),
    legend.labs = c("Tertile1: 0-1/3", "Tertile2: 1/3-2/3", "Tertile3: 2/3-1"),
    xlim = c(0, plot_x_scale)
  )+ labs( y = "Probability of No Event",
           x = "Days",
          title = paste("Biased ER Evaluation-",title_prefix)) 
  
  
   subtitle_text <- paste("event times(shape=",event.shape, ",scale=",event.scale, ",seed=",seed, ");\ncensor seed=",seed+6,"; Bootstrap seed =", id.seed)

   plot$plot <- plot$plot +
   labs(subtitle = subtitle_text) +
   theme(plot.subtitle = element_text(size = 10))+
   scale_x_continuous(breaks = seq(0, plot_x_scale, by = 50))
  # # Save the plot
   #ggsave(filename = save_path, plot = plot$plot, width = 6, height = 5, dpi = 400, units = "in")
  
  return(plot)
}

#3. KM plot for the merged dataset
KM_plot <- function(km_object, data, title_prefix,plot_x_scale = 600) {
    plot <- ggsurvplot(
    km_object,
    surv.median.line = "hv", # Add median survival lines
     color = "black", 
     conf.int.fill = "grey45",
     conf.int = TRUE,
     title = "Kaplan-Meier plot for merged dataset",
     xlab = "Days",
     ylab = "Probability of no event",
     xlim = c(0, plot_x_scale),
     # risk.table = "abs_pct",
     # risk.table.y.text = FALSE,
     cumevents = TRUE,
     cumcensor = TRUE,
     tables.theme = theme_cleantable(),
     tables.height = 0.15,
     break.time.by = 30
  )
  
    subtitle_text <- paste("event times has a Weibull distribution(shape=",event.shape, ",scale=",event.scale, ",seed=",seed, ");\ncensor times are randomly selected from 1:600 with =",seed+6,"; \nFix the length of dosing at 600 days, Bootstrap seed =",id.seed)
    
 plot$plot <- plot$plot +
   labs(subtitle = subtitle_text) + 
   theme(
    plot.title = element_text(size = 14), 
    plot.subtitle = element_text(size = 12), # Customize subtitle size as needed
    axis.title = element_text(size = 14), # Customize axis title size
    axis.text = element_text(size = 12) # Customize axis text size
  ) +
  scale_x_continuous(breaks = seq(0, plot_x_scale, by = 30))+
  theme(plot.title = element_text(hjust = 0.5))
  
  # # # Save the plot
  #  ggsave(filename = save_path, plot = plot$plot, width = 7, height = 5, dpi = 400, units = "in")
   
  return(plot)
}

#4. typical CP plot for DH3
Typical_plot_A1 <- function(data, y_var, title) {
  ggplot(data, aes(x = DAY, y = !!sym(y_var), color = as.factor(ID))) +
    geom_point() +
    geom_line() +
    theme_classic() +
    theme(
      panel.background = element_rect(fill = "white"),
      panel.grid.major.y = element_line(color = "grey"),
      panel.grid.minor.x = element_line(color = "grey", linetype = 2),
      panel.grid.major.x = element_line(color = "grey"),
      legend.background = element_rect(fill = "transparent"),
      plot.title = element_text(hjust = 0.5)
    ) +
    scale_color_brewer(palette = "Paired") +
    labs(x = "Time (days)",
         y = y_var,
         title = title,
         color = "ID")
}

```

##1.2 common function to manuplate the dataset

```{r}
#1. split the dataset into 3 groups(EO2)
tertile <- function(data, column_name, labels = c("1", "2", "3")) {
  # Calculate the tertiles
  tertiles <- quantile(data[[column_name]], probs = c(1/3, 2/3))
  
  # Categorize the column based on the tertiles
  categorized_column <- cut(data[[column_name]], 
                            breaks = c(-Inf, tertiles, Inf), 
                            labels = labels, 
                            include.lowest = TRUE)
  return(categorized_column)
}

#2.perfrom the Mann-Whitney U-Test and save the result
  
wilcox <- function(df_group1, df_group2, variables) {
  # Initialize an empty data frame to store results
  results_df <- data.frame(p_value = numeric(), stringsAsFactors = FALSE)
  
  for (var in variables) {
    # Run the Wilcoxon test for the current variable
    test_result <- wilcox.test(df_group1[[var]], df_group2[[var]], paired = FALSE, alternative = "two.sided",
                               conf.int = TRUE, conf.level=0.95) # Note: corrected "confi.level" to "conf.level"
    
    # Append the variable name and p-value to the results dataframe
     temp_result <- data.frame(P.Value = test_result$p.value, stringsAsFactors = FALSE)
     rownames(temp_result) <- var
     
     results_df <- rbind(results_df, temp_result)
  }
  return(results_df)
}

#3. extract results from univariate Cox PH models_continuous

univ_con <- function(univ_models) {
    univ_results <- lapply(univ_models, function(x) {
    x <- summary(x)  # Summarize the model
    # Extract and format the results
    P.value <- signif(x$wald["pvalue"], digits=3)
    wald.test <- signif(x$wald["test"], digits=3)
    beta <- signif(x$coef[1], digits=3)  # Coefficient beta
    HR <- signif(exp(x$coef[1]), digits=3)  # Hazard Ratio (HR)
    HR.confint.lower <- signif(x$conf.int[,"lower .95"], 3)
    HR.confint.upper <- signif(x$conf.int[,"upper .95"], 3)
    HR <- paste0(HR, " (",
                 HR.confint.lower, "-", HR.confint.upper, ")")
    res <- c(beta, HR, wald.test, P.value)
    names(res) <- c("beta", "HR (95% CI for HR)", "wald.test", "P.Value")
    return(res)
  })
  # Convert the list of results to a dataframe
  result_df <- t(as.data.frame(univ_results))
  return(result_df)
}

#4. extract results from univariate Cox PH models_categorical_4 groups (EO1)

univ_cat <- function(univ_models_G) {
  
          univ_results_G <- lapply(univ_models_G, function(x) { 
          x <- summary(x)

          # Extract Wald test p-value and statistic
          P.value<-signif(x$wald["pvalue"], digits=2)
          wald.test<-signif(x$wald["test"], digits=2)
          beta <- numeric(length = 3)
          HR <- numeric(length = 3)
          HR.confint.lower <- numeric(length = 3)
          HR.confint.upper <- numeric(length = 3)
          HRs <- character(length = 3)
  
          for (i in 1:3) {
            beta[i] <- signif(x$coef[i], digits = 3)
            HR[i] <- signif(exp(x$coef[i]), digits = 3) # HR
            HR.confint.lower[i] <- signif(x$conf.int[i,3],3)
            HR.confint.upper[i] <- signif(x$conf.int[i,4],3)
            HRs[i] <- paste0(HR[i], " (", HR.confint.lower[i], "-", HR.confint.upper[i], ")")
            }
           res <- c(beta, HRs, wald.test, P.value )
           names(res) <- c(paste0("Beta", 1:3), paste0("HR (95% CI) ", 1:3), "Wald.Test", "P.Value")
   
           return(res)

          })
          # Convert the list of results to a dataframe
          result_df <- t(as.data.frame(univ_results_G))
          return(result_df)
}  

# 5. extract results from univariate Cox PH models_categorical_3 GROUPs(EO2)

univ_cat_T <- function(univ_models_G) {
  
          univ_results_G <- lapply(univ_models_G, function(x) { 
          x <- summary(x)

          # Extract Wald test p-value and statistic
          P.value<-signif(x$wald["pvalue"], digits=2)
          wald.test<-signif(x$wald["test"], digits=2)
          beta <- numeric(length = 2)
          HR <- numeric(length = 2)
          HR.confint.lower <- numeric(length = 2)
          HR.confint.upper <- numeric(length = 2)
          HRs <- character(length = 2)
  
          for (i in 1:2) {
            beta[i] <- signif(x$coef[i], digits = 3)
            HR[i] <- signif(exp(x$coef[i]), digits = 3) # HR
            HR.confint.lower[i] <- signif(x$conf.int[i,3],3)
            HR.confint.upper[i] <- signif(x$conf.int[i,4],3)
            HRs[i] <- paste0(HR[i], " (", HR.confint.lower[i], "-", HR.confint.upper[i], ")")
            }
           res <- c(beta, HRs, wald.test, P.value )
           names(res) <- c(paste0("Beta", 1:2), paste0("HR (95% CI) ", 1:2), "Wald.Test", "P.Value")
   
           return(res)

          })
          # Convert the list of results to a dataframe
          result_df <- t(as.data.frame(univ_results_G))
          return(result_df)
}  


#6.extract results from univariate Cox PH models_continuous

logistic_analysis <- function(logistic_Results, covariates) {
  logistic_summaries <- lapply(logistic_Results, summary)
  
  logistic.results <- lapply(seq_along(logistic_summaries), function(i) {
    x <- logistic_summaries[[i]]
    P.value <- signif(x$coefficients[2, 4], digits=3)
    Slope <- signif(x$coefficients[2, 1], digits=3)
    AIC <- round(AIC(logistic_Results[[i]]), 1)   # Use the actual model object, not the summary
    res <- c(Slope, AIC, P.value)
    names(res) <- c("Slope","AIC", "P.Value")
    return(res)
  })
  
  # Convert the list of results to a dataframe
  result_df <- as.data.frame(do.call(rbind, logistic.results))
  row.names(result_df) <- covariates  # Add row names
  
  return(result_df)
}
```

#2 Generate the exposure dataset(df.exp)

Model:"KP_2com.mod"
1. input datasets: "dosing record for study 309.csv", "estimated individual PK parameters.csv".

2.Bootstrap 1000 individual PK parameters

3.adjusted parameters in order to simulate 3 different cases

-Case 1: Small molecule with a t1/2 ~ 7 to 8 h and tmax~3 h, dosed daily, reach to SS at day2; 
         Need to adjust KA*20, CL*18;
-Case 2: Small molecule with a t1/2 ~ 24 h and a tmax~ 5 h, dosed daily, reach to SS at day5;
         Need to adjust KA*20, CL*5;
-Case 3: Small molecule with a t1/2 ~ 2W, dosed every 2W or 4W, reach to SS at week10.
        Need to adjust KA*3, CL/120,V2/40,V3/20,Q/50;
        
4.Run the simulation (Dosage fix at 60 mg)

5.Extract the exposure matrix for Biased ER analysis

```{r echo=FALSE}
setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/Cabozantinib/ER analysis")
mod<- mread_cache("KP_2com.mod") 

# dose interval could be changed 
Case.fix <- 3
## Dose level for the 501 to 1000 ID 
FHDOSE <- 60
FMDOSE <- 40
FLDOSE <- 20

## Dose level for the 501 to 1000 ID 
HDOSE <- 5
MDOSE <- 2
LDOSE <- 1

# Generate dose_label
First_section_label <- c("0mg", paste0(c(FLDOSE, FMDOSE, FHDOSE), "mg"))
Second_section_label <- c("0mg", paste0(c(LDOSE, MDOSE, HDOSE), "mg"))

# Clear the input dose dataset
dose<- read.csv("dosing record for study 309.csv")
dose.451 <- dose %>%
        mutate(DOSE = as.numeric(DOSE))%>%
        select(-X9)%>%
        mutate(FORM = case_when(
               FORM == "TABLET" ~ 3, 
               FORM == "CAPSULE" ~ 2),
               DOSE = if_else(DOSE == 0, NA, DOSE))%>%
        mutate(DAY = as.numeric(DAY), 
               time = (DAY - 1) * 24,
               C = "") %>%
       dplyr::select(C, SUBJID,time, DAY, DOSE, FORM, SEQ) %>%   
       mutate( evid =1,
               amt=DOSE,
               cmt=1)%>% 
       mutate( SUBJID = as.numeric(SUBJID))

# random solected doing records based on ID
dose.seed = 2025
set.seed(dose.seed)
ID.dose <- unique(dose.451$SUBJID) 
dose.bootstrap <- sample(ID.dose, 1000, replace = TRUE)

dose.allID <- data.frame()

for (i in 1:length(dose.bootstrap)) {
  id <- dose.bootstrap[i]
  rows_for_id <- dose.451[dose.451$SUBJID == id, ]
  rows_for_id$NEWID <- i
  dose.allID <- bind_rows(dose.allID, rows_for_id)
}

dose.allID  <- dose.allID  %>%
               rename(ID=NEWID)%>%
               select(-SUBJID)
summary(dose.allID)

# random bootstrap the SUBJID with a fixed seed
parameter <- read.csv("estimated individual PK parameters.csv")
id.seed = 2025
set.seed(id.seed)
ID.unique <- unique(parameter$SUBJID) 
ID.bootstrap <- sample(ID.unique, 1000, replace = TRUE)

iparameter <- data.frame()

for (i in 1:length(ID.bootstrap)) {
  id <- ID.bootstrap[i]
  rows_for_id <- parameter[parameter$SUBJID == id, ]
  rows_for_id$NEWID <- i
  iparameter<- bind_rows(iparameter, rows_for_id)
}

iparameter <- iparameter %>%
               rename(ID=NEWID)


if (Case.fix == 1) {
    iparameter.allID <- iparameter%>%
                     mutate(KA=KA*20,
                     CL=CL*18)  

} else if (Case.fix == 2) {
    iparameter.allID <- iparameter%>%
                     mutate(KA=KA*20,
                     CL=CL*5)   
} else if (Case.fix == 3) {
    iparameter.allID <- iparameter%>%
                     mutate(KA=KA*3,
                     CL=CL/120, 
                     V2=V2/40,
                     V3=V3/30,
                     Q=Q/50)  
}
#hist(iparameter.allID$CL, main="Frequency of CL", ylab = "Frequency", col="orange")
#hist(iparameter.allID$V2, main="Frequency of V2", ylab = "Frequency", col="limegreen")

# run the simulation to get PK profiles
idata.allID <- left_join(dose.allID, iparameter.allID, by = "ID")

idata.allID = idata.allID[!is.na(idata.allID$DOSE), ]
# some IDs do not have estimated parameters
idata.allID = idata.allID[!is.na(idata.allID$KA), ]
#length(unique(idata.allID$ID)) 

idata.allID.2D <- idata.allID %>%
      mutate(  DOSE = case_when(
                         ID < 501 & DOSE == 60 ~ FHDOSE,
                         ID < 501 & DOSE == 40 ~ FMDOSE,
                         ID < 501 & DOSE == 20 ~ FLDOSE,
                         ID > 500 & DOSE == 60 ~ HDOSE,
                         ID > 500 & DOSE == 40 ~ MDOSE,
                         ID > 500 & DOSE == 20 ~ LDOSE,
                         TRUE ~ DOSE
         ))%>%
   mutate(amt=DOSE)

# ID_501 <- idata.allID.2D %>% filter(ID==49)

out <- mrgsim_df(mod, 
                 idata.allID.2D,
                 recover = c("CL","SUBJID"), 
                 carry.out = "EVID", 
                 recsort = 3,
                 atol = 1e-12, 
                 rtol = 1e-10,
                 tgrid = seq(0, 14400, by = 1))

exp <- out %>% 
  as_tibble() %>% 
  distinct() %>%
  group_by(time) %>%
  filter(EVID != 1) %>%
  ungroup()%>%
  mutate(DAY = ceiling(time/24),
         CP = if_else(CP < 0, 0, CP), # replace the negative CP to 0
         AUC = if_else(AUC < 0, 0,AUC))%>%  # replace the negative AUC to 0
  filter(time > 0)%>%
  mutate(Cavg = AUC/time)%>%  # average concentration time to event
  group_by(ID, DAY) %>%
  mutate(Cmax = max(CP),
         Ctrough = tail(CP,1)) %>%
  ungroup()

# adding DOSE and SEQ columns

Dose.IDindex <- idata.allID.2D %>%
    select(ID, DAY, DOSE, SEQ) %>%
    mutate(DOSE = if_else(is.na(DOSE), 0, DOSE))
    exp.0 <-  left_join(exp,Dose.IDindex, by = c("ID", "DAY"))


# clear the dataset 
#used to plot the first dosing interval profile
exp.1.hour<- exp.0 %>%
          select(-EVID,-GUT,-CENT,-PERIPH)%>%
          rename(AUCTE=AUC,
                 CavgTE= Cavg)%>%
          select(ID,SUBJID,DAY,time,DOSE,SEQ,CL,CP,Cmax,Ctrough,AUCTE,CavgTE)%>%
        #  filter(time %% 24 == 0)%>%
          mutate(DOSE = if_else(is.na(DOSE), 0, DOSE))
# used to conduct ER analysis 
exp.1<- exp.1.hour%>%
         filter(time %% 24 == 0)

# get the daily AUC
AUC1D <- exp.1 %>%
  arrange(ID, DAY) %>%
  group_by(ID) %>%
  # Calculate the last AUCTE for each day for each ID
  group_by(DAY, .add = TRUE) %>%
  summarize(LastAUCTE = last(AUCTE), .groups = 'drop_last') %>%
  # Calculate daily AUC for each ID
  group_by(ID) %>%
  mutate(AUC1D = LastAUCTE - lag(LastAUCTE, default=0)) %>%
  ungroup()%>%
  select(-LastAUCTE)%>%
  mutate(AUC1D = if_else(AUC1D < 0, 0,AUC1D))

Combined.data <- left_join(exp.1, AUC1D, by = c("ID", "DAY"))

# calculate all possible necessary exposure matrix
exp.2 <- Combined.data%>%
  group_by(ID) %>%
  mutate(AUC1W = rollsum(AUC1D, 7, fill = NA, align = "right"),
         AUC2W = rollsum(AUC1D, 14, fill = NA, align = "right"),
         AUC3W = rollsum(AUC1D, 21, fill = NA, align = "right"),
         AUC4W = rollsum(AUC1D, 28, fill = NA, align = "right"),
         Cavg = AUC1D / 24,
         Cavg1W = AUC1W / (24*7),
         Cavg2W = AUC2W / (24*14),
         Cavg3W = AUC3W / (24*21),
         Cavg4W = AUC4W / (24*28)) %>%
  ungroup()%>%
  mutate(AUC1W = ifelse(is.na(AUC1W), AUCTE, AUC1W),
         AUC2W = ifelse(is.na(AUC2W), AUCTE, AUC2W),
         AUC3W = ifelse(is.na(AUC3W), AUCTE, AUC3W),
         AUC4W = ifelse(is.na(AUC4W), AUCTE, AUC4W),
         Cavg1W = ifelse(is.na(Cavg1W), CavgTE, Cavg1W),
         Cavg2W = ifelse(is.na(Cavg2W), CavgTE, Cavg2W),
         Cavg3W = ifelse(is.na(Cavg3W), CavgTE, Cavg3W),
         Cavg4W = ifelse(is.na(Cavg4W), CavgTE, Cavg4W))

# only maintain the exposure matrix used for ER analysis
df.exp <- exp.2 %>%
           select(-time, -AUCTE,-AUC1D,-AUC1W,-AUC2W,-AUC3W,-AUC4W,-Cavg3W)

df.exp <- df.exp %>%
  group_by(ID) %>%
  mutate(Cavg1WC1 = if_else(DAY > 7, Cavg1W, Cavg1W[DAY == 7]),
         Cavg2WC1 = if_else(DAY > 14, Cavg2W, Cavg2W[DAY == 14]),
         Cavg4WC1 = if_else(DAY > 28, Cavg4W, Cavg4W[DAY == 28])) %>%
  ungroup()

```

#3 Simulate time to event dataset(TTE)
There scenarios will be explored:
- EO1: shape = 0.65, scale = 100;
- EO2: shape = 2, scale = 240;

75% of records are event times, which has a weibull distribution
- the hazard rate could be changed by using weibull distribution, 
- when shape < 1; the hazard rate will decrease with the time
- when shape = 1; it will reduce to exponential distribution, and has a constant hazard rate
- when shape > 1; hazard rate will increase with the time
- scale = 1/rate; used to adjust the hazard rate

25% of records are censor times, which are randomly selected with replacement from the follow-up period

Note: the maximum follow-up time is 600 days; the percentage of censor data is less than 10% before the median survial time.
```{r}
n.event = 750
n.censor= 250
plot_x_scale <- 600

seed <- 2024
event.shape <- 2
event.scale <- 240
set.seed(seed)
Sample.event <- tibble(ID = 1:n.event) %>% 
                mutate(time = 1 + ceiling(rweibull(n.event, shape = event.shape, scale = event.scale )),
                       event=1) 

set.seed(seed+6)
censor.time <- sample(1:600, n.censor, replace = TRUE)

Sample.censor <- tibble(ID = 1:n.censor) %>% 
                mutate(time = censor.time,
                       event=0) 

merge.data <- rbind(Sample.event, Sample.censor)

seed <- 2024
TTE <- merge.data %>% 
       mutate(event = if_else(time > 600, 0, event),
              ID = sample(1:1000, n(), replace = FALSE))%>% 
      arrange(ID)

```


#4 Create the df.ER dataset for biased ER analysis

- Create the df.ER dataset by integrating the TTE dataset into exposure metrics dataset(df.exp), aligning on time points.This process involves handling each ID individually and combining them sequentially. For each ID: 
 1. Extract all records for the selected ID;
 2. Determine the last day in the dosing record for this ID and compare it with its corresponding time point in the TTE dataset (for instance, ID=10 is matched with the TTE dataset's 10th time point);
 3. If the last dosing day is greater than or equal to the TTE time point, remove any data beyond this time and align the EVENT from TTE  with the new final day of the dosing record;
 4. If the last dosing day is less than the TTE time point, the EVENT is censored;
 
- df.ER includes all the data records
- analysis only include the last observation
## 4.1 Create df.ER & df.ER.IDQ
```{r}
# random bootstrap the SUBJID with a fixed seed
# merge the TTE dataset with df.ER dataset
df.ER <- data.frame()

for (i in 1:1000) {
  single <- TTE$time[i]
  rows_for_id <- df.exp[df.exp$ID == i, ]
  max_day <- max(rows_for_id$DAY, na.rm = TRUE)
  
  if (max_day < single) {
    rows_for_id$EVENT <- 0
    max_row <- which.max(rows_for_id$DAY)
    rows_for_id$LAST <- ifelse(1:nrow(rows_for_id) == max_row, 1, 0)
  } else {
    #Cut off the extra data that Day > single
    rows_for_id <- rows_for_id[rows_for_id$DAY <= single, ]
    max_row <- which.max(rows_for_id$DAY)
    #match the Event when TTE has censored values
    rows_for_id$EVENT <- ifelse(1:nrow(rows_for_id) == max_row, TTE$event[TTE$ID == i], 0)
    rows_for_id$LAST <- ifelse(1:nrow(rows_for_id) == max_row, 1, 0)
  }
   df.ER <- bind_rows(df.ER, rows_for_id)
}

analysis <- df.ER %>% filter(LAST == 1)
### prepare the df.ER.IDQ dataset 

# first day exposure
df.ER.ID <- df.ER %>%
            filter(SEQ == 1) %>%
            filter(DAY == 1) %>%
            select(SUBJID, ID, DOSE, CL, Ctrough, Cavg,Cavg2WC1,Cavg4WC1 )%>%
            rename(Ctrough1D=Ctrough,
                   Cavg1D=Cavg)
# last day CavgTE and EVENT & DAY
G.ID <-  analysis[,c("ID","DAY","Ctrough", "Cavg","Cavg1W","Cavg2W","Cavg4W","CavgTE", "EVENT","LAST")]%>%
         rename(CtroughOED=Ctrough,
                CavgOED=Cavg,
                Cavg1WOED=Cavg1W,
                Cavg2WOED=Cavg2W,
                Cavg4WOED=Cavg4W)

df.ER.IDQ <- merge(df.ER.ID, G.ID, by = "ID")%>%
             select(ID, SUBJID, DAY, DOSE, CL, LAST,EVENT, 
                    Ctrough1D, Cavg1D, CavgOED,Cavg1WOED,Cavg2WOED,Cavg4WOED,CavgTE)

```
##4.2 Check dataset
###4.2.1 Dose response
```{r}
# setwd( "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
# df.ER.IDQ <- read.csv("df_ER_IDQ.csv")
# df.exp <- readRDS("df_exp.rds")

df.ER.IDQ.100  <- df.ER.IDQ %>% filter(DOSE == 100)
df.ER.IDQ.5  <- df.ER.IDQ %>% filter(DOSE == 5)

df.ER.IDQ.df <- list(
  df.ER.IDQ.100 = df.ER.IDQ.100,
  df.ER.IDQ.5 = df.ER.IDQ.5
  )
# covariates <- c("Ctrough1D", "Cavg1D", "CtroughOED", "CavgOED", "Cavg1WOED", "CavgTE")
covariates <- c("CavgOED",  "CavgTE")
# Function to calculate event percentage and confidence interval for a single dataset
calculate_eff_stats <- function(data, covariates) {
  stats_list <- lapply(covariates, function(covariate_name) {
    # Calculate statistics for the current covariate
    data %>%
      summarise(
        N = n(),
        NResp = sum(EVENT == 1),
        # Minimum = min(!!sym(covariate_name), na.rm = TRUE),
        # Median = median(!!sym(covariate_name), na.rm = TRUE),
        # Maximum = max(!!sym(covariate_name), na.rm = TRUE),
        ObsProb = round(NResp / N, 4),
        .groups = "drop"
      ) %>%
      mutate(
        exact.lower = sapply(NResp, function(x) binom.test(x, N)$conf.int[1]),
        exact.upper = sapply(NResp, function(x) binom.test(x, N)$conf.int[2]),
        Covariate = covariate_name
      ) %>%
      as.data.frame()
  })
  
  # Combine results for all covariates
  do.call(rbind, stats_list)
}

# Combine results for all datasets
combined_results <- do.call(rbind, lapply(names(df.ER.IDQ.df), function(dataset_name) {
  data <- df.ER.IDQ.df[[dataset_name]]
  stats <- calculate_eff_stats(data, covariates)
  stats$Dataset <- dataset_name  # Add dataset name for identification
  stats
}))

# View the combined results
combined_results <- combined_results %>%
  select(Dataset, Covariate, everything()) %>%
  mutate(Dataset = factor(Dataset, levels = unique(Dataset)))


# Plot using ggplot2
DR_plot <- ggplot(combined_results, aes(x = Dataset, y = ObsProb)) +
  geom_point(size = 3, color = "blue") +  # Add points for ObsProb
  geom_errorbar(aes(ymin = exact.lower, ymax = exact.upper), width = 0.2, color = "black") +  # Add error bars
  facet_wrap(~ Covariate, scales = "free_y") +  # Create facets for each Covariate
  custom_theme +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels
    strip.text = element_text(size = 12)  # Adjust facet label size
  ) +
  labs(
    x = "Dataset",
    y = "Observed Probability",
    title = "Dose Response"
  )+
  theme(plot.title = element_text(size = 14, hjust = 0.5))

ggsave("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/Dose_response.tiff",
      plot = DR_plot,
     width = 5, height = 4, dpi = 400, units = "in") 
```

###4.2.2 KM plot stratified by the starting dose level
```{r}
KM_plot_S <- function(km_object, data, title_prefix, plot_x_scale = 600,save_path) {
  plot <- ggsurvplot(
    km_object,
    data = data,
    surv.median.line = "hv",
    conf.int = TRUE,
    pval = TRUE,
    pval.method = TRUE,
    risk.table = TRUE,
    risk.table.y.text = FALSE,
    tables.theme = theme_cleantable(),
    tables.height = 0.25,
    palette = c("red", "blue2"),
    xlim = c(0, plot_x_scale)
  )+ labs( y = "Probability of No Event",
           x = "Days",
          title = paste("Biased ER Evaluation-",title_prefix)) 
  # # Save the plot
   ggsave(filename = save_path, plot = plot$plot, width = 6, height = 5, dpi = 400, units = "in")
  
  return(plot)
}

df.ER.IDQ <- df.ER.IDQ %>% mutate(DOSE = factor(DOSE))

KM.DOSE <- survfit(Surv(DAY, EVENT) ~ DOSE, data = df.ER.IDQ )

plot.DOSE <- KM_plot_S(KM.DOSE, df.ER.IDQ,"DOSE",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_stratified by dose.tiff")
```

###4.2.3 Dose history
```{r}

custom_theme <- theme_classic() +
  theme(
    panel.grid.major = element_line(color = "gray70", linewidth = 0.5),  # Major grid lines
    panel.grid.minor = element_line(color = "gray90", linewidth = 0.25)  # Minor grid lines
  )
  
# for the Higher starting dose
 # Count and prepare totals
 EventDATA <- df.ER.IDQ %>% filter(ID<501) %>% select(DAY, EVENT)%>% arrange(DAY)
 Utime <- unique(EventDATA$DAY)
  
# Number at risk and number of events at each time
 ni <- sapply(Utime, function(t) sum(EventDATA$DAY >= t))
 di <- sapply(Utime, function(t) sum(EventDATA$DAY == t & EventDATA$EVENT == 1))
 Event_probs <- 1-cumprod(1-di/ni)

CumEvent <- data.frame(DAY = Utime, PEvent = Event_probs* 100)
     
# Count and prepare totals
DosehistG <- df.exp %>%
  filter(ID<501) %>%
  select(ID, DOSE,DAY) %>%
  count(DAY, DOSE, name = "freq") %>%
  group_by(DAY) %>%
  mutate(TOTAL = sum(freq)) %>%
  ungroup()%>%
  mutate(percentage = (freq / TOTAL) * 100)%>%
  mutate(DOSE = as.factor(DOSE))


DE.plot <- ggplot() +
   geom_col(data = CumEvent, aes(x = DAY, y = PEvent), color = "grey40", fill = "grey40", alpha = 0.01) + 
   geom_line(data = DosehistG, aes(x = DAY, y = percentage, group = DOSE, color = DOSE), size = 1) +
   scale_x_continuous(name = "Days since first dose") +
   scale_y_continuous(name = "Failure probability", sec.axis = sec_axis(~ . * 1, name = "% of dose level")) +
   ggtitle("Dose vs event summary plot") +
   scale_color_discrete(name = "Dose", labels = First_section_label) +
   custom_theme +
   theme(legend.title = element_blank(),
         legend.text = element_text(size = 12),
         plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
         axis.text.x = element_text(size = 10),
         axis.text.y = element_text(size = 10),
         axis.title.x = element_text(hjust = 0.5, size = 12),
         axis.title.y = element_text(hjust = 0.5, size = 12))

DE.plot

DE.PLOT2<- DE.plot + xlim(0, 150)

ggsave("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/Dose vs event summary plot_low.tiff",
             plot = DE.plot, 
             width = 5, height = 3, dpi = 400, units = "in")

# for low starting dose
 # Count and prepare totals
 EventDATA <- df.ER.IDQ %>% filter(ID>500)%>% select(DAY, EVENT) %>% arrange(DAY)
 Utime <- unique(EventDATA$DAY)
  
# Number at risk and number of events at each time
 ni <- sapply(Utime, function(t) sum(EventDATA$DAY >= t))
 di <- sapply(Utime, function(t) sum(EventDATA$DAY == t & EventDATA$EVENT == 1))
 Event_probs <- 1-cumprod(1-di/ni)

CumEvent <- data.frame(DAY = Utime, PEvent = Event_probs* 100)
     
# Count and prepare totals
DosehistG <- df.exp %>%
  filter(ID>500) %>%
  select(ID, DOSE,DAY) %>%
  count(DAY, DOSE, name = "freq") %>%
  group_by(DAY) %>%
  mutate(TOTAL = sum(freq)) %>%
  ungroup()%>%
  mutate(percentage = (freq / TOTAL) * 100)%>%
  mutate(DOSE = as.factor(DOSE))


DE.plot <- ggplot() +
   geom_col(data = CumEvent, aes(x = DAY, y = PEvent), color = "grey40", fill = "grey40", alpha = 0.01) + 
   geom_line(data = DosehistG, aes(x = DAY, y = percentage, group = DOSE, color = DOSE), size = 1) +
   scale_x_continuous(name = "Days since first dose") +
   scale_y_continuous(name = "Failure probability", sec.axis = sec_axis(~ . * 1, name = "% of dose level")) +
   ggtitle("Dose vs event summary plot") +
   scale_color_discrete(name = "Dose", labels = Second_section_label) +
   custom_theme +
   theme(legend.title = element_blank(),
         legend.text = element_text(size = 12),
         plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
         axis.text.x = element_text(size = 10),
         axis.text.y = element_text(size = 10),
         axis.title.x = element_text(hjust = 0.5, size = 12),
         axis.title.y = element_text(hjust = 0.5, size = 12))

DE.plot

DE.PLOT2<- DE.plot + xlim(0, 150)

ggsave("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/Dose vs event summary plot_high.tiff",
             plot = DE.plot, 
             width = 5, height = 3, dpi = 400, units = "in")
```

#5 Conventional ER analysis
##5.1 Creat KM plots (no strata)
```{r}
# KM plot for the merged dataset
plot_x_scale <- 600
time <- analysis$DAY
event <- analysis$EVENT

KM_fit <- survfit(Surv(time, event) ~ 1, data = analysis )
plot<- KM_plot(KM_fit, analysis, "merged dataset", 600)

p1 = plot$plot
p2 = plot$cumevents
p3 = plot$ncensor.plot
plotp = cowplot::plot_grid(p1,p2,p3,align = "v",ncol =1,rel_heights = c(4,1,1))

ggsave("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM plot_no strata.tiff",
        plotp, 
        width = 7, height = 5, dpi = 300, units = "in")
```
##5.2 KM Plot based on the grouped ID 
### quantiled exposure metrics (3 groups)

```{r}
df.ER.IDQ$Ctrough1D_IDQ  <- tertile(df.ER.IDQ, "Ctrough1D")
df.ER.IDQ$Cavg1D_IDQ  <- tertile(df.ER.IDQ, "Cavg1D")
df.ER.IDQ$CavgOED_IDQ  <- tertile(df.ER.IDQ, "CavgOED")
df.ER.IDQ$Cavg1WOED_IDQ  <- tertile(df.ER.IDQ, "Cavg1WOED")
df.ER.IDQ$Cavg2WOED_IDQ  <- tertile(df.ER.IDQ, "Cavg2WOED")
df.ER.IDQ$Cavg4WOED_IDQ  <- tertile(df.ER.IDQ, "Cavg4WOED")
df.ER.IDQ$CavgTE_IDQ  <- tertile(df.ER.IDQ, "CavgTE")

KM.Ctrough1DIDQ <- survfit(Surv(DAY, EVENT) ~ Ctrough1D_IDQ, data = df.ER.IDQ )
KM.Cavg1DIDQ <- survfit(Surv(DAY, EVENT) ~ Cavg1D_IDQ, data = df.ER.IDQ )
KM.CavgOEDIDQ <- survfit(Surv(DAY, EVENT) ~ CavgOED_IDQ, data = df.ER.IDQ)
KM.Cavg1WOEDIDQ <- survfit(Surv(DAY, EVENT) ~ Cavg1WOED_IDQ, data = df.ER.IDQ )
KM.Cavg2WOEDIDQ <- survfit(Surv(DAY, EVENT) ~ Cavg2WOED_IDQ, data = df.ER.IDQ )
KM.Cavg4WOEDIDQ <- survfit(Surv(DAY, EVENT) ~ Cavg4WOED_IDQ, data = df.ER.IDQ )
KM.CavgTEIDQ <- survfit(Surv(DAY, EVENT) ~ CavgTE_IDQ, data = df.ER.IDQ ) 

plot1 <- KM_plot_T(KM.Ctrough1DIDQ, df.ER.IDQ,"Ctrough1D_IDQ",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_False_Ctrough1D.tiff")

plot2 <- KM_plot_T(KM.Cavg1DIDQ, df.ER.IDQ,"Cavg1D_IDQ",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_False_Cavg1D.tiff")

plot3 <- KM_plot_T(KM.CavgOEDIDQ, df.ER.IDQ,"CavgOED_IDQ",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_False_CavgOED.tiff")
plot4 <- KM_plot_T(KM.Cavg1WOEDIDQ, df.ER.IDQ,"Cavg1WOED_IDQ",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_False_Cavg1W.tiff")

plot5 <- KM_plot_T(KM.Cavg2WOEDIDQ, df.ER.IDQ,"Cavg2WOED_IDQ",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_False_Cavg2W.tiff")

plot6 <- KM_plot_T(KM.CavgTEIDQ, df.ER.IDQ,"CavgTE_IDQ",600,
       "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM_False_CavgTE.tiff")

combine_3 <- grid.arrange(plot1$plot, plot2$plot, plot3$plot, plot4$plot,plot5$plot, plot6$plot, ncol = 3)

ggsave("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/KM Plot 3 groups.tiff",
      plot = combine_3, 
     width = 18, height = 10, dpi = 400, units = "in")

setwd( "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10") 
write.csv(df.ER.IDQ, "df_ER_IDQ.csv", row.names = FALSE)
saveRDS(df.exp, file = "df_exp.rds")
```

##5.3 logistic regression plots

```{r}
plot1 <- logistic_regression_plot(df.ER.IDQ, "Ctrough1D", "EVENT")
plot2 <- logistic_regression_plot(df.ER.IDQ, "Cavg1D", "EVENT")
plot3 <- logistic_regression_plot(df.ER.IDQ, "CavgOED", "EVENT")
plot4 <- logistic_regression_plot(df.ER.IDQ, "Cavg1WOED", "EVENT")
plot5 <- logistic_regression_plot(df.ER.IDQ, "Cavg2WOED", "EVENT")
plot6 <- logistic_regression_plot(df.ER.IDQ, "CavgTE", "EVENT")

combine_2 <- grid.arrange(plot1, plot2, plot3, plot4,plot5,plot6, ncol = 3)

ggsave("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/Logistic regression plot_conc.tiff",
      plot = combine_2, 
     width = 12, height = 6, dpi = 400, units = "in")
```
##5.4 Conventional ER analysis tests
### wilcox, Univariate CoxPH, logistic regression
```{r}
#1. Perform Wilcox Test for the exposures grouped by IDs 
## get the mean values for the exposures
ID.e <- df.ER.IDQ %>% filter(EVENT==1)
ID.n <- df.ER.IDQ %>% filter(EVENT==0)

covariates <- colnames(df.ER.IDQ)[c(5,8:14)]
wilcox_result <- wilcox(ID.e, ID.n, covariates)%>%rownames_to_column(var = "Variable")

#2. univariate Cox PH regression_continuous covariate

univ_formulas <- sapply(covariates, function(x) as.formula(paste('Surv(DAY, EVENT) ~', x)))
univ_models <- lapply(univ_formulas, function(x) { coxph(x, data = df.ER.IDQ) })
univ_con_result <- univ_con(univ_models)%>%
                   as.data.frame() %>%
                  rownames_to_column(var = "Variable")


#3. logistic regression analysis

logistic_formulas <- sapply(covariates, function(x) as.formula(paste('EVENT ~', x)))
logistic_Results <- lapply(logistic_formulas, function(x) glm(x, family = binomial(logit), data = df.ER.IDQ))
logistic_result <- logistic_analysis(logistic_Results,covariates)%>%
                    rownames_to_column(var = "Variable")

### univariate Cox PH regression_categorical covariate
#### EO2 (3 groups)

covariates_G <- colnames(df.ER.IDQ)[c(15:21)]

univ_formulas_G <- sapply(covariates_G, function(x) as.formula(paste('Surv(DAY, EVENT) ~', x)))
univ_models_G <- lapply(univ_formulas_G, function(x) { coxph(x, data = df.ER.IDQ) })
univ_cat_result <- univ_cat_T( univ_models_G)%>%
   as.data.frame() %>%
  rownames_to_column(var = "Variable")

wb <- createWorkbook()
addWorksheet(wb, "Logistic Result")
addWorksheet(wb, "CoxPH Conc Result")
addWorksheet(wb, "CoxPH cate Result")
addWorksheet(wb, "wilcox result")

writeData(wb, sheet = "Logistic Result", x = logistic_result)
writeData(wb, sheet = "CoxPH Conc Result", x = univ_con_result)
writeData(wb, sheet = "CoxPH cate Result", x = univ_cat_result)
writeData(wb, sheet = "wilcox result", x = wilcox_result)

variables_to_abstract <- colnames(df.ER.IDQ)[c(8:12,14)]

logistic_result_abstract <- logistic_result %>% filter (Variable%in% variables_to_abstract)
CoxPH_conc_abstract <- univ_con_result %>% filter (Variable%in% variables_to_abstract)%>% select(Variable,beta,P.Value)

wb_abstract <- createWorkbook()
addWorksheet(wb_abstract, "Logistic Result Abstract")
addWorksheet(wb_abstract, "CoxPH Conc Abstract")
writeData(wb_abstract, sheet = "Logistic Result Abstract", x = logistic_result_abstract)
writeData(wb_abstract, sheet = "CoxPH Conc Abstract", x = CoxPH_conc_abstract)

setwd( "C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10") 
saveWorkbook(wb, file = "Conventional ER methods result.xlsx", overwrite = TRUE)
saveWorkbook(wb_abstract, file = "CoxPH&logistic abstract result.xlsx", overwrite = TRUE)
```
#6 Replicate simulation_EO1

## 6.1 common function

```{r}
#1. loop function
loop <- function(DI, Case.fix, seed, parameter.451, dose.451, event.shape, event.scale){

# random solected doing records based on ID

       set.seed(seed-10)
       ID.dose <- unique(dose.451$SUBJID) 
       dose.bootstrap <- sample(ID.dose, 1000, replace = TRUE)

       dose.allID <- data.frame()

       for (i in 1:length(dose.bootstrap)) {
         id <- dose.bootstrap[i]
         rows_for_id <- dose.451[dose.451$SUBJID == id, ]
         rows_for_id$NEWID <- i
         dose.allID <- bind_rows(dose.allID, rows_for_id)
       }

       dose.allID  <- dose.allID  %>%
                      rename(ID=NEWID)%>%
                     select(-SUBJID)

# random bootstrap the SUBJID with a fixed seed
       seed1 <- seed+6
       set.seed(seed1)
       ID.unique <- unique(parameter.451$SUBJID) 
       ID.bootstrap <- sample(ID.unique, 1000, replace = TRUE)

       iparameter <- data.frame()

       for (i in 1:length(ID.bootstrap)) {
         id <- ID.bootstrap[i]
         rows_for_id <- parameter.451[parameter.451$SUBJID == id, ]
         rows_for_id$NEWID <- i
         iparameter<- bind_rows(iparameter, rows_for_id)
       }

       iparameter <- iparameter %>%
                      rename(ID=NEWID)


       if (Case.fix == 1) {
           iparameter.allID <- iparameter%>%
                            mutate(KA=KA*20,
                            CL=CL*18)  

       } else if (Case.fix == 2) {
           iparameter.allID <- iparameter%>%
                            mutate(KA=KA*20,
                            CL=CL*5)   
       } else if (Case.fix == 3) {
           iparameter.allID <- iparameter%>%
                     mutate(KA=KA*3,
                     CL=CL/120, 
                     V2=V2/40,
                     V3=V3/30,
                     Q=Q/50)  
       }

# run the simulation to get PK profiles
       idata.allID <- left_join(dose.allID, iparameter.allID, by = "ID")
       idata.allID = idata.allID[!is.na(idata.allID$DOSE), ]
# some IDs do not have estimated parameters
       idata.allID = idata.allID[!is.na(idata.allID$KA), ]

        idata.allID.2D <- idata.allID %>%
              mutate(  DOSE = case_when(
                         ID < 501 & DOSE == 60 ~ FHDOSE,
                         ID < 501 & DOSE == 40 ~ FMDOSE,
                         ID < 501 & DOSE == 20 ~ FLDOSE,
                         ID > 500 & DOSE == 60 ~ HDOSE,
                         ID > 500 & DOSE == 40 ~ MDOSE,
                         ID > 500 & DOSE == 20 ~ LDOSE,
                         TRUE ~ DOSE
              ))%>%
       mutate(amt=DOSE)

       out <- mrgsim_df(mod, 
                        idata.allID.2D,
                        recover = c("CL","SUBJID"), 
                        carry.out = "EVID", 
                        recsort = 3,
                        # atol = 1e-12, 
                        # rtol = 1e-10,
                        tgrid = seq(0, 14400, by = 1))

       exp <- out %>% 
         as_tibble() %>% 
         distinct() %>%
         group_by(time) %>%
         filter(EVID != 1) %>%
         ungroup()%>%
         mutate(DAY = ceiling(time/24),
                CP = if_else(CP < 0, 0, CP), # replace the negative CP to 0
                AUC = if_else(AUC < 0, 0,AUC))%>%  # replace the negative AUC to 0
         filter(time > 0)%>%
         mutate(Cavg = AUC/time)%>%  # average concentration time to event
         group_by(ID, DAY) %>%
         mutate(Cmax = max(CP),
                Ctrough = tail(CP,1)) %>%
         ungroup()

# adding DOSE and SEQ columns
       Dose.IDindex <- idata.allID.2D  %>%
           select(ID, DAY, DOSE, SEQ) %>%
           mutate(DOSE = if_else(is.na(DOSE), 0, DOSE))

       exp.0 <-  left_join(exp,Dose.IDindex, by = c("ID", "DAY"))
# clear the dataset 
#used to plot the first dosing interval profile
       exp.1 <- exp.0 %>%
                 filter(time %% 24 == 0)%>%
                 select(-EVID,-GUT,-CENT,-PERIPH)%>%
                 rename(AUCTE=AUC,
                        CavgTE= Cavg)%>%
                 select(ID,SUBJID,DAY,time,DOSE,SEQ,CL,CP,Cmax,Ctrough,AUCTE,CavgTE)%>%
                 mutate(DOSE = if_else(is.na(DOSE), 0, DOSE))

# get the daily AUC
       AUC1D <- exp.1 %>%
         arrange(ID, DAY) %>%
         group_by(ID) %>%
         group_by(DAY, .add = TRUE) %>%
         summarize(LastAUCTE = last(AUCTE), .groups = 'drop_last') %>%
         group_by(ID) %>%
         mutate(AUC1D = LastAUCTE - lag(LastAUCTE, default=0)) %>%
         ungroup()%>%
         select(-LastAUCTE)%>%
         mutate(AUC1D = if_else(AUC1D < 0, 0,AUC1D))

       Combined.data <- left_join(exp.1, AUC1D, by = c("ID", "DAY"))

# calculate all possible necessary exposure matrix
       df.exp <- Combined.data%>%
                group_by(ID) %>%
                mutate(AUC1W = rollsum(AUC1D, 7, fill = NA, align = "right"),
                AUC2W = rollsum(AUC1D, 14, fill = NA, align = "right"),
                AUC3W = rollsum(AUC1D, 21, fill = NA, align = "right"),
                AUC4W = rollsum(AUC1D, 28, fill = NA, align = "right"),
                Cavg = AUC1D / 24,
                Cavg1W = AUC1W / (24*7),
                Cavg2W = AUC2W / (24*14),
                Cavg3W = AUC3W / (24*21),
                Cavg4W = AUC4W / (24*28)) %>%
         ungroup()%>%
         mutate(AUC1W = ifelse(is.na(AUC1W), AUCTE, AUC1W),
                AUC2W = ifelse(is.na(AUC2W), AUCTE, AUC2W),
                AUC3W = ifelse(is.na(AUC3W), AUCTE, AUC3W),
                AUC4W = ifelse(is.na(AUC4W), AUCTE, AUC4W),
                Cavg1W = ifelse(is.na(Cavg1W), CavgTE, Cavg1W),
                Cavg2W = ifelse(is.na(Cavg2W), CavgTE, Cavg2W),
                Cavg3W = ifelse(is.na(Cavg3W), CavgTE, Cavg3W),
                Cavg4W = ifelse(is.na(Cavg4W), CavgTE, Cavg4W))%>%
         select(-time, -AUCTE,-AUC1D,-AUC1W,-AUC2W,-AUC3W,-AUC4W,-Cavg3W)%>%
                 group_by(ID) %>%
                 mutate(Cavg1WC1 = if_else(DAY > 7, Cavg1W, Cavg1W[DAY == 7]),
                        Cavg2WC1 = if_else(DAY > 14, Cavg2W, Cavg2W[DAY == 14]),
                        Cavg4WC1 = if_else(DAY > 28, Cavg4W, Cavg4W[DAY == 28])) %>%
                 ungroup()

   #Simulate TTE dataset
       set.seed(seed)
       n.event = 750
       n.censor= 250
       Sample.event <- tibble(ID = 1:n.event) %>% 
                mutate(time = 1 + ceiling(rweibull(n.event, shape = event.shape, scale = event.scale )),
                       event=1) 

      set.seed(seed+6)
      censor.time <- sample(1:600, n.censor, replace = TRUE)

      Sample.censor <- tibble(ID = 1:n.censor) %>% 
                       mutate(time = censor.time, event=0) 

      merge.data <- rbind(Sample.event, Sample.censor)
      
      set.seed(seed)
      TTE <- merge.data %>% 
             mutate(event = if_else(time > 600, 0, event),
                    ID = sample(1:1000, n(), replace = FALSE))%>% 
            arrange(ID)

      # merge the TTE dataset with df.ER dataset
       df.ER <- data.frame()

      for (i in 1:1000) {
        single <- TTE$time[i]
        rows_for_id <- df.exp[df.exp$ID == i, ]
        max_day <- max(rows_for_id$DAY, na.rm = TRUE)
  
        if (max_day < single) {
          rows_for_id$EVENT <- 0
          max_row <- which.max(rows_for_id$DAY)
          rows_for_id$LAST <- ifelse(1:nrow(rows_for_id) == max_row, 1, 0)
        } else {
    #Cut off the extra data that Day > single
          rows_for_id <- rows_for_id[rows_for_id$DAY <= single, ]
          max_row <- which.max(rows_for_id$DAY)
    #match the Event when TTE has censored values
          rows_for_id$EVENT <- ifelse(1:nrow(rows_for_id) == max_row, TTE$event[TTE$ID == i], 0)
          rows_for_id$LAST <- ifelse(1:nrow(rows_for_id) == max_row, 1, 0)
        }
         df.ER <- bind_rows(df.ER, rows_for_id)
      }


      analysis <- df.ER %>% filter(LAST == 1)

        # ================only Using first day data points =====================
        ## Group the IDs based on the quantiled exposure at the first day(df.ER.IDQ)
            df.ER.ID <- df.ER %>%
            filter(SEQ == 1) %>%
            filter(DAY == 1) %>%
            select(SUBJID, ID, DOSE, CL, Ctrough, Cavg,Cavg2WC1,Cavg4WC1 )%>%
            rename(Ctrough1D=Ctrough,
                   Cavg1D=Cavg)
          # last day CavgTE and EVENT & DAY
           G.ID <-  analysis[,c("ID","DAY","Ctrough", "Cavg","Cavg1W","Cavg2W","Cavg4W","CavgTE", "EVENT","LAST")]%>%
                    rename(CtroughOED=Ctrough,
                           CavgOED=Cavg,
                           Cavg1WOED=Cavg1W,
                           Cavg2WOED=Cavg2W,
                           Cavg4WOED=Cavg4W)

           df.ER.IDQ <- merge(df.ER.ID, G.ID, by = "ID")%>%
                        select(ID, SUBJID, DAY, DOSE, CL, LAST,EVENT, 
                               Ctrough1D, Cavg1D, CavgOED,Cavg1WOED,Cavg2WOED,Cavg4WOED,CavgTE)


               df.ER.IDQ$Ctrough1D_IDQ  <- tertile(df.ER.IDQ, "Ctrough1D")
               df.ER.IDQ$Cavg1D_IDQ  <- tertile(df.ER.IDQ, "Cavg1D")
               df.ER.IDQ$CavgOED_IDQ  <- tertile(df.ER.IDQ, "CavgOED")
               df.ER.IDQ$Cavg1WOED_IDQ  <- tertile(df.ER.IDQ, "Cavg1WOED")
               df.ER.IDQ$Cavg2WOED_IDQ  <- tertile(df.ER.IDQ, "Cavg2WOED")
               df.ER.IDQ$Cavg4WOED_IDQ  <- tertile(df.ER.IDQ, "Cavg4WOED")
               df.ER.IDQ$CavgTE_IDQ  <- tertile(df.ER.IDQ, "CavgTE")
               
               #1. Perform Wilcox Test for the exposures grouped by IDs 
                ID.e <- df.ER.IDQ %>% filter(EVENT==1)
                ID.n <- df.ER.IDQ %>% filter(EVENT==0)

                covariates <- colnames(df.ER.IDQ)[c(5,8:14)]
                wilcox_result <- wilcox(ID.e, ID.n, covariates)

               #2. univariate Cox PH regression_continuous covariate

               univ_formulas <- sapply(covariates, function(x) as.formula(paste('Surv(DAY, EVENT) ~', x)))
               univ_models <- lapply(univ_formulas, function(x) { coxph(x, data = df.ER.IDQ) })
               univ_con_result <- univ_con(univ_models)

               #3. logistic regression analysis

               logistic_formulas <- sapply(covariates, function(x) as.formula(paste('EVENT ~', x)))
               logistic_Results <- lapply(logistic_formulas, function(x) glm(x, family = binomial(logit), data = df.ER.IDQ))
               logistic_result <- logistic_analysis(logistic_Results,covariates)
               ### univariate Cox PH regression_categorical covariate
               #### EO2 (3 groups)

               covariates_G <- colnames(df.ER.IDQ)[c(15:21)]

               univ_formulas_G <- sapply(covariates_G, function(x) as.formula(paste('Surv(DAY, EVENT) ~', x)))
               univ_models_G <- lapply(univ_formulas_G, function(x) { coxph(x, data = df.ER.IDQ) })
               univ_cat_result <- univ_cat_T( univ_models_G)
           
 
      return(list(result_1 = univ_con_result, result_2 = logistic_result,result_3 = univ_cat_result,wilcox_results = wilcox_result ))
      
}

#2. abstract P.Value values from the list
extract_pvalue <- function(list_data, target_row_name) {
    # Initialize an empty dataframe to store results
    results <- data.frame(row_name = character(), P.Value = numeric(), Seed = character(), stringsAsFactors = FALSE)
    
    # Iterate over each list element using its name
    list_names <- names(list_data)
    for (seed in list_names) {
        # Check if 'target_row_name' is a row name in the data frame
        if(target_row_name %in% rownames(list_data[[seed]])) {
            # Extract the P.Value for the given row name
            p_value <- as.numeric(list_data[[seed]][target_row_name, "P.Value", drop = TRUE])
            
            # Combine the extracted P.Value with the current seed name into results
            temp_result <- data.frame(row_name = target_row_name, P.Value = p_value, Seed = seed, stringsAsFactors = FALSE)
            results <- rbind(results, temp_result)
        }
    }
    return(results)
}

###3. common function to abstract P.Values from the list & split the p-values into two groups 
# - P.Value >= 0.05, group1=0; P.Value < 0.05, group1 =1
# - P.Value >= 0.01, group2=0; P.Value < 0.01, group2 =1

process_dataset <- function(data, variables, type) {
 
  results_list <- list()
  for (variable in variables) {
    extracted <- extract_pvalue(data, variable)
    
    # Assign group based on p-value
    extracted$Group1 <- ifelse(extracted$P.Value >= 0.05, "p>=0.05", "p<0.05")
    extracted$Group2 <- ifelse(extracted$P.Value >= 0.01, "p>=0.01", "p<0.01")
    
    # Generate summary table for both groups
    group1_table <- table(extracted$Group1)
    group2_table <- table(extracted$Group2)
    
    # Add tables to the results list
    results_list[[paste0(type, "_", variable, "_Group1")]] <- group1_table
    results_list[[paste0(type, "_", variable, "_Group2")]] <- group2_table
  }
  return(results_list)
}

#4. abstract slope values from logistic regression results
extract_slope <- function(list_data, Slope, variables) {
  # Initialize an empty list to store the results for each variable
  results_list <- list()
  
  # Iterate over each variable in the provided variables
  for (variable in variables) {
    # Initialize an empty dataframe to store results for the current variable
    results <- data.frame(row_name = character(), Slope.Value = numeric(), Seed = character(), stringsAsFactors = FALSE)
    
    # Iterate over each list element using its name (seed)
    for (seed in names(list_data)) {
      # Check if the 'variable' is a row name in the data frame of the current seed
      if (variable %in% rownames(list_data[[seed]])) {
        # Extract the Slope value for the given variable
        Slope_value <- as.numeric(list_data[[seed]][variable, Slope, drop = TRUE])
        
        # Combine the extracted Slope value with the current seed name into results
        temp_result <- data.frame(row_name = variable, Slope.Value = Slope_value, Seed = seed, stringsAsFactors = FALSE)
        results <- rbind(results, temp_result)
      }
    }
    
    # Store the results for the current variable in the results list
    results_list[[variable]] <- results
  }
  
  return(results_list)
}

#5. summrize slope values from logistic regression
summarize_slopes <- function(slope_results) {
  # Initialize an empty data frame to store the summary results
  summary_results <- data.frame(
    row_name = character(),
    Logit.Min = numeric(),
    Logit.Max = numeric(),
    Logit.Mean = numeric(),
    Logit.Median = numeric(),
    Logit.q0.025 = numeric(),
    Logit.q0.975 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Iterate over each con_var in the slope_results
  for (variable in names(slope_results)) {
    # Extract the data frame for the current variable
    variable_data <- slope_results[[variable]]
    
    # Group by row_name and compute summary statistics
    grouped_summary <- variable_data %>%
      dplyr::group_by(row_name) %>%
      dplyr::summarize(
        Logit.Min = min(Slope.Value, na.rm = TRUE),
        Logit.Max = max(Slope.Value, na.rm = TRUE),
        Logit.Mean = mean(Slope.Value, na.rm = TRUE),
        Logit.Median = median(Slope.Value, na.rm = TRUE),
        Logit.q0.025 = quantile(Slope.Value, 0.025, na.rm = TRUE),
        Logit.q0.975 = quantile(Slope.Value, 0.975, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Add the summaries for the current variable to the result data frame
    summary_results <- rbind(summary_results, grouped_summary)
  }
  
  return(summary_results)
}
#6. extracting beta values for continuous covariates

extract_beta_cont <- function(list_data, target_row_names, group_count) {
    # Initialize an empty dataframe to store results
    results <- data.frame(row_name = character(), Beta = numeric(), Seed = character(), stringsAsFactors = FALSE)
    
    # Iterate over each list element using its name
    list_names <- names(list_data)
    for (seed in list_names) {
        for (target_row_name in target_row_names) {
            # Check if 'target_row_name' is a row name in the data frame
            if(target_row_name %in% rownames(list_data[[seed]])) {
                # Extract the beta Value for the given row name
                beta <- as.numeric(list_data[[seed]][target_row_name, "beta", drop = TRUE])
                
                # Combine the extracted Beta Value with the current seed name into results
                temp_result <- data.frame(row_name = target_row_name, Beta = beta, Seed = seed, stringsAsFactors = FALSE)
                results <- rbind(results, temp_result)
            }
        }
    }
    return(results)
}

#7. extracting beta values for categorical covariates
extract_beta_Cate <- function(list_data, target_row_names, group_count) {
 
  if(!(group_count %in% c(3, 4))) {
    stop("group_count must be either 3 or 4")
  }
  
  initial_columns <- c("row_name", "Seed")
  
  # Dynamically adjust the beta columns based on group_count
  beta_columns <- if(group_count == 4) {
    c("Beta1", "Beta2", "Beta3")
  } else {
    c("Beta1", "Beta2")
  }
  
  # Combine initial columns with beta columns for the complete dataframe structure
  results_columns <- c(initial_columns, beta_columns)
  results <- setNames(data.frame(matrix(ncol = length(results_columns), nrow = 0)), results_columns)
  
  # Iterate over each list element using its name
  list_names <- names(list_data)
  for (seed in list_names) {
    for (target_row_name in target_row_names) {
      # Check if 'target_row_name' is a row name in the data frame
      if(target_row_name %in% rownames(list_data[[seed]])) {
        # Initialize a vector for the beta values
        beta_values <- numeric(length(beta_columns))
        
        # Extract the Beta Values for the given row name based on group_count
        for (i in 1:length(beta_columns)) {
          beta_value <- as.numeric(list_data[[seed]][target_row_name, beta_columns[i], drop = TRUE])
          beta_values[i] <- beta_value
        }
        
        # Combine the extracted Beta Values with the current seed name into results
        temp_result <- c(target_row_name, seed, beta_values)
        temp_df <- setNames(data.frame(as.list(temp_result), stringsAsFactors = FALSE), results_columns)
        results <- rbind(results, temp_df)
      }
    }
  }
  return(results)
}

#8 common function for summarzing beta values for categorical covariates

summarize_beta_cate <- function(Beta.cate, group_count) {
  # Ensure Beta.cate has the expected beta columns for the specified group_count
  expected_beta_cols <- paste0("Beta", 1:(group_count - 1))
  Beta.cate <- Beta.cate %>% select(row_name, Seed, all_of(expected_beta_cols))

  # Convert Beta columns to long format
  Beta.cate_long <- Beta.cate %>%
    pivot_longer(cols = starts_with("Beta"), names_to = "Beta_variable", values_to = "Beta_value") %>%
    mutate(Beta_value = as.numeric(Beta_value), # Ensure Beta_value is numeric
           Beta_variable = factor(Beta_variable, levels = expected_beta_cols))
  
  # Summarize Beta_value by Beta_variable and row_name
  Beta.cate_summary <- Beta.cate_long %>%
    group_by(row_name, Beta_variable) %>%
    summarise(
      Cox.Min = min(Beta_value, na.rm = TRUE),
      Cox.Max = max(Beta_value, na.rm = TRUE),
      Cox.Mean = mean(Beta_value, na.rm = TRUE),
      Cox.Median = median(Beta_value, na.rm = TRUE),
      Cox.q0.025 = quantile(Beta_value, 0.025, na.rm = TRUE),
      Cox.q0.975 = quantile(Beta_value, 0.975, na.rm = TRUE),
      .groups = "drop"
    )

  return(Beta.cate_summary)
}

#9. sumarize odd ratios
logit_odd <- function(logit_results) {
  # Initialize an empty data frame to store the summary results
  summary_results <- data.frame(
    row_name = character(),
    Logit.Min = numeric(),
    Logit.Max = numeric(),
    Logit.Mean = numeric(),
    Logit.Median = numeric(),
    Logit.q0.025 = numeric(),
    Logit.q0.975 = numeric(),
    stringsAsFactors = FALSE
  )
  
  for (variable in names(logit_results)) {
    # Extract the data frame for the current variable
    variable_data <- logit_results[[variable]]
    
    # Group by row_name and compute summary statistics
    grouped_summary <- variable_data %>%
      dplyr::group_by(row_name) %>%
      dplyr::summarize(
        Logit.Min = min(Odd.ratio, na.rm = TRUE),
        Logit.Max = max(Odd.ratio, na.rm = TRUE),
        Logit.Mean = mean(Odd.ratio, na.rm = TRUE),
        Logit.Median = median(Odd.ratio, na.rm = TRUE),
        Logit.q0.025 = quantile(Odd.ratio, 0.025, na.rm = TRUE),
        Logit.q0.975 = quantile(Odd.ratio, 0.975, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Add the summaries for the current variable to the result data frame
    summary_results <- rbind(summary_results, grouped_summary)
  }
  
  return(summary_results)
}
```


##6.2 Using random seed to perform the ER analysis
### input the initial dose and parameter dataset
```{r}
setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis")
# mrgsolve model
mod<- mread_cache("KP_2com.mod") 
# Clear the input dose dataset
parameter.451 <- read.csv("estimated individual PK parameters.csv")

dose<- read.csv("dosing record for study 309.csv")
dose.451 <- dose %>%
        mutate(DOSE = as.numeric(DOSE))%>%
        select(-X9)%>%
        mutate(FORM = case_when(
               FORM == "TABLET" ~ 3, 
               FORM == "CAPSULE" ~ 2),
               DOSE = if_else(DOSE == 0, NA, DOSE))%>%
        mutate(DAY = as.numeric(DAY), 
               time = (DAY - 1) * 24,
               C = "") %>%
       dplyr::select(C, SUBJID,time, DAY, DOSE, FORM, SEQ) %>%   
       mutate( evid =1,
               amt=DOSE,
               cmt=1)%>% 
       mutate( SUBJID = as.numeric(SUBJID))
```
There scenarios will be explored:
- EO1: shape = 0.65, scale = 100;
- EO2: shape = 2, scale = 240;
### run the simulation
```{r}
# dose interval could be changed 
DI = 1 # DAY
Case.fix <- 1
event.shape <-0.65
event.scale <- 100
## The 1 to 500 ID has 60&40&20 dose level
FHDOSE <- 60
FMDOSE <- 40
FLDOSE <- 20
## The 501 to 1000 ID has 20&10&5 dose level
HDOSE <- 10
MDOSE <- 5
LDOSE <- 2

# Initialize lists to store the results from each simulation
univ_con_IDQ <- list()
univ_cat_IDQ <- list()
logistic_test <- list()
wilcox_test <- list()
# Set a fixed seed for reproducibility
set.seed(2025)
# Randomly select 100 seeds from 1 to 100,000
seeds <- sample(10:10000, 1000)
#loop <- function(seed, df.exp, shape, scale, group_count){
for (seed in seeds) {
  simulation_result <- loop(DI, Case.fix, seed, parameter.451, dose.451, event.shape, event.scale) 
  univ_con_IDQ[[as.character(seed)]] <- simulation_result$result_1
  logistic_test[[as.character(seed)]] <- simulation_result$result_2
  univ_cat_IDQ[[as.character(seed)]] <- simulation_result$result_3
  wilcox_test[[as.character(seed)]] <- simulation_result$wilcox_results
 }
setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveRDS(univ_con_IDQ, "univ_con_IDQ.rds")
saveRDS(univ_cat_IDQ, "univ_cat_IDQ.rds")
saveRDS(wilcox_test, "wilcox_test.rds")
saveRDS(logistic_test, "logistic_test.rds")
```

## read saved files if need to get more information
```{r}
univ_con_IDQ <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/univ_con_IDQ.rds")
univ_cat_IDQ <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/univ_cat_IDQ.rds")
wilcox_test <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/wilcox_test.rds")
logistic_test <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/logistic_test.rds")
#univ_con_IDQ[1]
DI=1
```
##6.3 Abstract P-values
```{r}
# Specify variables for each type of analysis
  con_var <- c("Cavg1D", "CavgOED","Cavg1WOED","Cavg2WOED","Cavg4WOED", "CavgTE")
  cat_var <-  c("Cavg1D_IDQ","CavgOED_IDQ","Cavg1WOED_IDQ","Cavg2WOED_IDQ","Cavg4WOED_IDQ", "CavgTE_IDQ")
  Wilcox_var <- c("Cavg1D", "CavgOED","Cavg1WOED","Cavg2WOED","Cavg4WOED", "CavgTE")

cat_IDQ <- process_dataset(univ_cat_IDQ, cat_var, "cat")
con_IDQ <- process_dataset(univ_con_IDQ, con_var, "con")
Wilcox <- process_dataset(wilcox_test, Wilcox_var, "WTest")
logist <- process_dataset(logistic_test, con_var, "Logist")

tables_list <- c(con_IDQ,logist)
#tables_list <- c(con_IDQ ,cat_IDQ,  Wilcox)

final_table <- bind_rows(tables_list, .id = "Source")

table_0_05 <- final_table %>%
  filter(grepl("Group1", Source)) %>%
 # select(-c(`p>=0.01`, `p<0.01`)) %>%
  mutate_all(~replace(., is.na(.), 0))

```
##6.4  abstract and summary slope values for logistic regression
```{r}
slope_results <- extract_slope(logistic_test, Slope = "Slope", variables = con_var)
summary_results <- summarize_slopes(slope_results)

logit_results <- lapply(slope_results, function(df) {
  df$Odd.ratio <- exp(df$Slope.Value) # Add the new column
  return(df) # Return the updated data frame
})

summary_results <- summarize_slopes(slope_results)
setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveRDS(logit_results, "logit results.rds")
```

##6.5  abstract and summary beta values 

```{r}
Beta.cont <- extract_beta_cont(univ_con_IDQ, con_var)
coxPH_odd <- Beta.cont %>% mutate(hazard.ratio = exp(Beta))

Beta.cate <- extract_beta_Cate(univ_cat_IDQ, cat_var,3) 

Beta.cont_df <- Beta.cont %>%
  group_by(row_name) %>%
  summarise(
    Cox.Min = min(Beta),
    Cox.Max = max(Beta),
    Cox.Mean = mean(Beta),
    Cox.Median = median(Beta),
    Cox.q0.025 = quantile(Beta, 0.025), 
    Cox.q0.975 = quantile(Beta, 0.975)   
    )%>%
   mutate(Beta_variable = "Beta")

Beta.cate_df <- summarize_beta_cate(Beta.cate, 3)

Beta.summary <- bind_rows(Beta.cont_df, Beta.cate_df)

variable.count<- length(con_var)
Beta.filtered <- Beta.summary[1:variable.count, ]

p.filtered_con <- table_0_05[1:variable.count, ]
p.filtered_logit <- table_0_05[(variable.count+1):(2*variable.count), ]

p_values_con <- p.filtered_con %>%
  select(Source, `p<0.05`) %>%
  mutate(Source = str_replace(Source, "^con_", "")) %>%
  mutate(Source = str_replace(Source, "_Group1$", ""))%>%
  rename(row_name=Source)%>%
  rename(`Cox(P<0.05)` = `p<0.05`)

p_values_logit <- p.filtered_logit %>%
  select(Source, `p<0.05`) %>%
  mutate(Source = str_replace(Source, "^Logist_", "")) %>%
  mutate(Source = str_replace(Source, "_Group1$", ""))%>%
  rename(row_name=Source)%>%
  rename(`Logit(P<0.05)` = `p<0.05`)

loop.summary.logit <-  left_join(p_values_logit, summary_results, by = "row_name")
loop.summary.cox <-  left_join(p_values_con, Beta.filtered, by = "row_name")

loop.summary.logit<- loop.summary.logit %>% rename(Covariate = row_name)
loop.summary.cox<- loop.summary.cox %>% rename(Covariate = row_name) %>%select(-Beta_variable)

wb_summary <- createWorkbook()
addWorksheet(wb_summary, "Logistic Analysis")
addWorksheet(wb_summary, "Univariate CoxPH Analysis")
writeData(wb_summary, sheet = "Logistic Analysis", x = loop.summary.logit)
writeData(wb_summary, sheet = "Univariate CoxPH Analysis", x = loop.summary.cox)

setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveWorkbook(wb_summary, file = "Loop Results_Slope&Beta.xlsx", overwrite = TRUE)
saveRDS(coxPH_odd, "CoxPH results.rds")

logit_odd_results <- logit_odd(logit_results)

Cox_odd_results <- coxPH_odd %>%
  group_by(row_name) %>%
  summarise(
    Cox.Min = min(hazard.ratio),
    Cox.Max = max(hazard.ratio),
    Cox.Mean = mean(hazard.ratio),
    Cox.Median = median(hazard.ratio),
    Cox.q0.025 = quantile(hazard.ratio, 0.025), 
    Cox.q0.975 = quantile(hazard.ratio, 0.975)  
    )

loop.summary.logit <-  left_join(p_values_logit, logit_odd_results, by = "row_name")
loop.summary.cox <-  left_join(p_values_con, Cox_odd_results, by = "row_name")

wb_summary <- createWorkbook()
addWorksheet(wb_summary, "Logistic Analysis")
addWorksheet(wb_summary, "Univariate CoxPH Analysis")
writeData(wb_summary, sheet = "Logistic Analysis", x = loop.summary.logit)
writeData(wb_summary, sheet = "Univariate CoxPH Analysis", x = loop.summary.cox)

setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveWorkbook(wb_summary, file = "Loop Results_odd&hazard_ratio.xlsx", overwrite = TRUE)
```
#7  For PK1 EO2 having breaks issue_did not run CoxPH catergorical
##7.1 common function
```{r}
#1. loop function
loop <- function(DI, Case.fix, seed, parameter.451, dose.451, event.shape, event.scale){

# random solected doing records based on ID

       set.seed(seed-10)
       ID.dose <- unique(dose.451$SUBJID) 
       dose.bootstrap <- sample(ID.dose, 1000, replace = TRUE)

       dose.allID <- data.frame()

       for (i in 1:length(dose.bootstrap)) {
         id <- dose.bootstrap[i]
         rows_for_id <- dose.451[dose.451$SUBJID == id, ]
         rows_for_id$NEWID <- i
         dose.allID <- bind_rows(dose.allID, rows_for_id)
       }

       dose.allID  <- dose.allID  %>%
                      rename(ID=NEWID)%>%
                     select(-SUBJID)

# random bootstrap the SUBJID with a fixed seed
       seed1 <- seed+6
       set.seed(seed1)
       ID.unique <- unique(parameter.451$SUBJID) 
       ID.bootstrap <- sample(ID.unique, 1000, replace = TRUE)

       iparameter <- data.frame()

       for (i in 1:length(ID.bootstrap)) {
         id <- ID.bootstrap[i]
         rows_for_id <- parameter.451[parameter.451$SUBJID == id, ]
         rows_for_id$NEWID <- i
         iparameter<- bind_rows(iparameter, rows_for_id)
       }

       iparameter <- iparameter %>%
                      rename(ID=NEWID)


       if (Case.fix == 1) {
           iparameter.allID <- iparameter%>%
                            mutate(KA=KA*20,
                            CL=CL*18)  

       } else if (Case.fix == 2) {
           iparameter.allID <- iparameter%>%
                            mutate(KA=KA*20,
                            CL=CL*5)   
       } else if (Case.fix == 3) {
           iparameter.allID <- iparameter%>%
                     mutate(KA=KA*3,
                     CL=CL/120, 
                     V2=V2/40,
                     V3=V3/30,
                     Q=Q/50)  
       }

# run the simulation to get PK profiles
       idata.allID <- left_join(dose.allID, iparameter.allID, by = "ID")
       idata.allID = idata.allID[!is.na(idata.allID$DOSE), ]
# some IDs do not have estimated parameters
       idata.allID = idata.allID[!is.na(idata.allID$KA), ]
       HDOSE <- 20
       MDOSE <- 10
       LDOSE <- 5

        idata.allID.2D <- idata.allID %>%
              mutate(  DOSE = case_when(
                         ID < 501 & DOSE == 60 ~ FHDOSE,
                         ID < 501 & DOSE == 40 ~ FMDOSE,
                         ID < 501 & DOSE == 20 ~ FLDOSE,
                         ID > 500 & DOSE == 60 ~ HDOSE,
                         ID > 500 & DOSE == 40 ~ MDOSE,
                         ID > 500 & DOSE == 20 ~ LDOSE,
                         TRUE ~ DOSE
              ))%>%
       mutate(amt=DOSE)

       out <- mrgsim_df(mod, 
                        idata.allID.2D,
                        recover = c("CL","SUBJID"), 
                        carry.out = "EVID", 
                        recsort = 3,
                        # atol = 1e-12, 
                        # rtol = 1e-10,
                        tgrid = seq(0, 14400, by = 1))

       exp <- out %>% 
         as_tibble() %>% 
         distinct() %>%
         group_by(time) %>%
         filter(EVID != 1) %>%
         ungroup()%>%
         mutate(DAY = ceiling(time/24),
                CP = if_else(CP < 0, 0, CP), # replace the negative CP to 0
                AUC = if_else(AUC < 0, 0,AUC))%>%  # replace the negative AUC to 0
         filter(time > 0)%>%
         mutate(Cavg = AUC/time)%>%  # average concentration time to event
         group_by(ID, DAY) %>%
         mutate(Cmax = max(CP),
                Ctrough = tail(CP,1)) %>%
         ungroup()

# adding DOSE and SEQ columns
       Dose.IDindex <- idata.allID.2D %>%
           select(ID, DAY, DOSE, SEQ) %>%
           mutate(DOSE = if_else(is.na(DOSE), 0, DOSE))

       exp.0 <-  left_join(exp,Dose.IDindex, by = c("ID", "DAY"))
# clear the dataset 
#used to plot the first dosing interval profile
       exp.1 <- exp.0 %>%
                 filter(time %% 24 == 0)%>%
                 select(-EVID,-GUT,-CENT,-PERIPH)%>%
                 rename(AUCTE=AUC,
                        CavgTE= Cavg)%>%
                 select(ID,SUBJID,DAY,time,DOSE,SEQ,CL,CP,Cmax,Ctrough,AUCTE,CavgTE)%>%
                 mutate(DOSE = if_else(is.na(DOSE), 0, DOSE))

# get the daily AUC
       AUC1D <- exp.1 %>%
         arrange(ID, DAY) %>%
         group_by(ID) %>%
         group_by(DAY, .add = TRUE) %>%
         summarize(LastAUCTE = last(AUCTE), .groups = 'drop_last') %>%
         group_by(ID) %>%
         mutate(AUC1D = LastAUCTE - lag(LastAUCTE, default=0)) %>%
         ungroup()%>%
         select(-LastAUCTE)%>%
         mutate(AUC1D = if_else(AUC1D < 0, 0,AUC1D))

       Combined.data <- left_join(exp.1, AUC1D, by = c("ID", "DAY"))

# calculate all possible necessary exposure matrix
       df.exp <- Combined.data%>%
                group_by(ID) %>%
                mutate(AUC1W = rollsum(AUC1D, 7, fill = NA, align = "right"),
                AUC2W = rollsum(AUC1D, 14, fill = NA, align = "right"),
                AUC3W = rollsum(AUC1D, 21, fill = NA, align = "right"),
                AUC4W = rollsum(AUC1D, 28, fill = NA, align = "right"),
                Cavg = AUC1D / 24,
                Cavg1W = AUC1W / (24*7),
                Cavg2W = AUC2W / (24*14),
                Cavg3W = AUC3W / (24*21),
                Cavg4W = AUC4W / (24*28)) %>%
         ungroup()%>%
         mutate(AUC1W = ifelse(is.na(AUC1W), AUCTE, AUC1W),
                AUC2W = ifelse(is.na(AUC2W), AUCTE, AUC2W),
                AUC3W = ifelse(is.na(AUC3W), AUCTE, AUC3W),
                AUC4W = ifelse(is.na(AUC4W), AUCTE, AUC4W),
                Cavg1W = ifelse(is.na(Cavg1W), CavgTE, Cavg1W),
                Cavg2W = ifelse(is.na(Cavg2W), CavgTE, Cavg2W),
                Cavg3W = ifelse(is.na(Cavg3W), CavgTE, Cavg3W),
                Cavg4W = ifelse(is.na(Cavg4W), CavgTE, Cavg4W))%>%
         select(-time, -AUCTE,-AUC1D,-AUC1W,-AUC2W,-AUC3W,-AUC4W,-Cavg3W)%>%
                 group_by(ID) %>%
                 mutate(Cavg1WC1 = if_else(DAY > 7, Cavg1W, Cavg1W[DAY == 7]),
                        Cavg2WC1 = if_else(DAY > 14, Cavg2W, Cavg2W[DAY == 14]),
                        Cavg4WC1 = if_else(DAY > 28, Cavg4W, Cavg4W[DAY == 28])) %>%
                 ungroup()


   #Simulate TTE dataset
       set.seed(seed)
       n.event = 750
       n.censor= 250
       Sample.event <- tibble(ID = 1:n.event) %>% 
                mutate(time = 1 + ceiling(rweibull(n.event, shape = event.shape, scale = event.scale )),
                       event=1) 

      set.seed(seed+6)
      censor.time <- sample(1:600, n.censor, replace = TRUE)

      Sample.censor <- tibble(ID = 1:n.censor) %>% 
                       mutate(time = censor.time,
                       event=0) 

      merge.data <- rbind(Sample.event, Sample.censor)
      set.seed(seed)
      TTE <- merge.data %>% 
             mutate(event = if_else(time > 600, 0, event),
                    ID = sample(1:1000, n(), replace = FALSE))%>% 
            arrange(ID)

      # merge the TTE dataset with df.ER dataset
       df.ER <- data.frame()

      for (i in 1:1000) {
        single <- TTE$time[i]
        rows_for_id <- df.exp[df.exp$ID == i, ]
        max_day <- max(rows_for_id$DAY, na.rm = TRUE)
  
        if (max_day < single) {
          rows_for_id$EVENT <- 0
          max_row <- which.max(rows_for_id$DAY)
          rows_for_id$LAST <- ifelse(1:nrow(rows_for_id) == max_row, 1, 0)
        } else {
    #Cut off the extra data that Day > single
          rows_for_id <- rows_for_id[rows_for_id$DAY <= single, ]
          max_row <- which.max(rows_for_id$DAY)
    #match the Event when TTE has censored values
          rows_for_id$EVENT <- ifelse(1:nrow(rows_for_id) == max_row, TTE$event[TTE$ID == i], 0)
          rows_for_id$LAST <- ifelse(1:nrow(rows_for_id) == max_row, 1, 0)
        }
         df.ER <- bind_rows(df.ER, rows_for_id)
      }


      analysis <- df.ER %>%
          filter(LAST == 1)

        # ================only Using first day data points =====================
        ## Group the IDs based on the quantiled exposure at the first day(df.ER.IDQ)
            df.ER.ID <- df.ER %>%
            filter(SEQ == 1) %>%
            filter(DAY == 1) %>%
            select(SUBJID, ID, DOSE, CL, Ctrough, Cavg,Cavg2WC1,Cavg4WC1 )%>%
            rename(Ctrough1D=Ctrough,
                   Cavg1D=Cavg)
          # last day CavgTE and EVENT & DAY
           G.ID <-  analysis[,c("ID","DAY","Ctrough", "Cavg","Cavg1W","Cavg2W","Cavg4W","CavgTE", "EVENT","LAST")]%>%
                    rename(CtroughOED=Ctrough,
                           CavgOED=Cavg,
                           Cavg1WOED=Cavg1W,
                           Cavg2WOED=Cavg2W,
                           Cavg4WOED=Cavg4W)

           df.ER.IDQ <- merge(df.ER.ID, G.ID, by = "ID")%>%
                        select(ID, SUBJID, DAY, DOSE, CL, LAST,EVENT, 
                               Ctrough1D, Cavg1D, CavgOED,Cavg1WOED,Cavg2WOED,Cavg4WOED,CavgTE)


               #1. Perform Wilcox Test for the exposures grouped by IDs 
                ID.e <- df.ER.IDQ %>% filter(EVENT==1)
                ID.n <- df.ER.IDQ %>% filter(EVENT==0)

                covariates <- colnames(df.ER.IDQ)[c(5,8:14)]
                wilcox_result <- wilcox(ID.e, ID.n, covariates)

               #2. univariate Cox PH regression_continuous covariate

               univ_formulas <- sapply(covariates, function(x) as.formula(paste('Surv(DAY, EVENT) ~', x)))
               univ_models <- lapply(univ_formulas, function(x) { coxph(x, data = df.ER.IDQ) })
               univ_con_result <- univ_con(univ_models)

               #3. logistic regression analysis

               logistic_formulas <- sapply(covariates, function(x) as.formula(paste('EVENT ~', x)))
               logistic_Results <- lapply(logistic_formulas, function(x) glm(x, family = binomial(logit), data = df.ER.IDQ))
               logistic_result <- logistic_analysis(logistic_Results,covariates)
               ### univariate Cox PH regression_categorical covariate
               #### EO2 (3 groups)

      return(list(result_1 = univ_con_result, result_2 = logistic_result, wilcox_results = wilcox_result ))        
      # return(list(df.exp = df.exp,result_1 = univ_con_result, result_2 = logistic_result, wilcox_results = wilcox_result ))
      
} 

# 2.abstract P.Value values from the list

extract_pvalue <- function(list_data, target_row_name) {
    # Initialize an empty dataframe to store results
    results <- data.frame(row_name = character(), P.Value = numeric(), Seed = character(), stringsAsFactors = FALSE)
    
    # Iterate over each list element using its name
    list_names <- names(list_data)
    for (seed in list_names) {
        # Check if 'target_row_name' is a row name in the data frame
        if(target_row_name %in% rownames(list_data[[seed]])) {
            # Extract the P.Value for the given row name
            p_value <- as.numeric(list_data[[seed]][target_row_name, "P.Value", drop = TRUE])
            
            # Combine the extracted P.Value with the current seed name into results
            temp_result <- data.frame(row_name = target_row_name, P.Value = p_value, Seed = seed, stringsAsFactors = FALSE)
            results <- rbind(results, temp_result)
        }
    }
    return(results)
}

###3. common function to abstract P.Values from the list & split the p-values into two groups 
# - P.Value >= 0.05, group1=0; P.Value < 0.05, group1 =1
# - P.Value >= 0.01, group2=0; P.Value < 0.01, group2 =1
process_dataset <- function(data, variables, type) {
 
  results_list <- list()
  for (variable in variables) {
    extracted <- extract_pvalue(data, variable)
    
    # Assign group based on p-value
    extracted$Group1 <- ifelse(extracted$P.Value >= 0.05, "p>=0.05", "p<0.05")
    extracted$Group2 <- ifelse(extracted$P.Value >= 0.01, "p>=0.01", "p<0.01")
    
    # Generate summary table for both groups
    group1_table <- table(extracted$Group1)
    group2_table <- table(extracted$Group2)
    
    # Add tables to the results list
    results_list[[paste0(type, "_", variable, "_Group1")]] <- group1_table
    results_list[[paste0(type, "_", variable, "_Group2")]] <- group2_table
  }
  return(results_list)
}

#4. abstract slope values from logistic regression results
extract_slope <- function(list_data, Slope, variables) {
  # Initialize an empty list to store the results for each variable
  results_list <- list()
  
  # Iterate over each variable in the provided variables
  for (variable in variables) {
    # Initialize an empty dataframe to store results for the current variable
    results <- data.frame(row_name = character(), Slope.Value = numeric(), Seed = character(), stringsAsFactors = FALSE)
    
    # Iterate over each list element using its name (seed)
    for (seed in names(list_data)) {
      # Check if the 'variable' is a row name in the data frame of the current seed
      if (variable %in% rownames(list_data[[seed]])) {
        # Extract the Slope value for the given variable
        Slope_value <- as.numeric(list_data[[seed]][variable, Slope, drop = TRUE])
        
        # Combine the extracted Slope value with the current seed name into results
        temp_result <- data.frame(row_name = variable, Slope.Value = Slope_value, Seed = seed, stringsAsFactors = FALSE)
        results <- rbind(results, temp_result)
      }
    }
    
    # Store the results for the current variable in the results list
    results_list[[variable]] <- results
  }
  
  return(results_list)
}

#5. summrize slope values from logistic regression
summarize_slopes <- function(slope_results) {
  # Initialize an empty data frame to store the summary results
  summary_results <- data.frame(
    row_name = character(),
    Logit.Min = numeric(),
    Logit.Max = numeric(),
    Logit.Mean = numeric(),
    Logit.Median = numeric(),
    Logit.q0.025 = numeric(),
    Logit.q0.975 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Iterate over each con_var in the slope_results
  for (variable in names(slope_results)) {
    # Extract the data frame for the current variable
    variable_data <- slope_results[[variable]]
    
    # Group by row_name and compute summary statistics
    grouped_summary <- variable_data %>%
      dplyr::group_by(row_name) %>%
      dplyr::summarize(
        Logit.Min = min(Slope.Value, na.rm = TRUE),
        Logit.Max = max(Slope.Value, na.rm = TRUE),
        Logit.Mean = mean(Slope.Value, na.rm = TRUE),
        Logit.Median = median(Slope.Value, na.rm = TRUE),
        Logit.q0.025 = quantile(Slope.Value, 0.025, na.rm = TRUE),
        Logit.q0.975 = quantile(Slope.Value, 0.975, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Add the summaries for the current variable to the result data frame
    summary_results <- rbind(summary_results, grouped_summary)
  }
  
  return(summary_results)
}

#6 extracting beta values for continuous covariates

extract_beta_cont <- function(list_data, target_row_names, group_count) {
    # Initialize an empty dataframe to store results
    results <- data.frame(row_name = character(), Beta = numeric(), Seed = character(), stringsAsFactors = FALSE)
    
    # Iterate over each list element using its name
    list_names <- names(list_data)
    for (seed in list_names) {
        for (target_row_name in target_row_names) {
            # Check if 'target_row_name' is a row name in the data frame
            if(target_row_name %in% rownames(list_data[[seed]])) {
                # Extract the beta Value for the given row name
                beta <- as.numeric(list_data[[seed]][target_row_name, "beta", drop = TRUE])
                
                # Combine the extracted Beta Value with the current seed name into results
                temp_result <- data.frame(row_name = target_row_name, Beta = beta, Seed = seed, stringsAsFactors = FALSE)
                results <- rbind(results, temp_result)
            }
        }
    }
    return(results)
}

#7 extracting beta values for categorical covariates
extract_beta_Cate <- function(list_data, target_row_names, group_count) {
 
  if(!(group_count %in% c(3, 4))) {
    stop("group_count must be either 3 or 4")
  }
  
  initial_columns <- c("row_name", "Seed")
  
  # Dynamically adjust the beta columns based on group_count
  beta_columns <- if(group_count == 4) {
    c("Beta1", "Beta2", "Beta3")
  } else {
    c("Beta1", "Beta2")
  }
  
  # Combine initial columns with beta columns for the complete dataframe structure
  results_columns <- c(initial_columns, beta_columns)
  results <- setNames(data.frame(matrix(ncol = length(results_columns), nrow = 0)), results_columns)
  
  # Iterate over each list element using its name
  list_names <- names(list_data)
  for (seed in list_names) {
    for (target_row_name in target_row_names) {
      # Check if 'target_row_name' is a row name in the data frame
      if(target_row_name %in% rownames(list_data[[seed]])) {
        # Initialize a vector for the beta values
        beta_values <- numeric(length(beta_columns))
        
        # Extract the Beta Values for the given row name based on group_count
        for (i in 1:length(beta_columns)) {
          beta_value <- as.numeric(list_data[[seed]][target_row_name, beta_columns[i], drop = TRUE])
          beta_values[i] <- beta_value
        }
        
        # Combine the extracted Beta Values with the current seed name into results
        temp_result <- c(target_row_name, seed, beta_values)
        temp_df <- setNames(data.frame(as.list(temp_result), stringsAsFactors = FALSE), results_columns)
        results <- rbind(results, temp_df)
      }
    }
  }
  return(results)
}

#8 common function for summarzing beta values for categorical covariates

summarize_beta_cate <- function(Beta.cate, group_count) {
  # Ensure Beta.cate has the expected beta columns for the specified group_count
  expected_beta_cols <- paste0("Beta", 1:(group_count - 1))
  Beta.cate <- Beta.cate %>% select(row_name, Seed, all_of(expected_beta_cols))

  # Convert Beta columns to long format
  Beta.cate_long <- Beta.cate %>%
    pivot_longer(cols = starts_with("Beta"), names_to = "Beta_variable", values_to = "Beta_value") %>%
    mutate(Beta_value = as.numeric(Beta_value), # Ensure Beta_value is numeric
           Beta_variable = factor(Beta_variable, levels = expected_beta_cols))
  
  # Summarize Beta_value by Beta_variable and row_name
  Beta.cate_summary <- Beta.cate_long %>%
    group_by(row_name, Beta_variable) %>%
    summarise(
      Cox.Min = min(Beta_value, na.rm = TRUE),
      Cox.Max = max(Beta_value, na.rm = TRUE),
      Cox.Mean = mean(Beta_value, na.rm = TRUE),
      Cox.Median = median(Beta_value, na.rm = TRUE),
      Cox.q0.025 = quantile(Beta_value, 0.025), 
      Cox.q0.975 = quantile(Beta_value, 0.975), 
      .groups = "drop"
    )

  return(Beta.cate_summary)
}

#9. sumarize odd ratios
logit_odd <- function(logit_results) {
  # Initialize an empty data frame to store the summary results
  summary_results <- data.frame(
    row_name = character(),
    Logit.Min = numeric(),
    Logit.Max = numeric(),
    Logit.Mean = numeric(),
    Logit.Median = numeric(),
    Logit.q0.025 = numeric(),
    Logit.q0.975 = numeric(),
    stringsAsFactors = FALSE
  )
  
  for (variable in names(logit_results)) {
    # Extract the data frame for the current variable
    variable_data <- logit_results[[variable]]
    
    # Group by row_name and compute summary statistics
    grouped_summary <- variable_data %>%
      dplyr::group_by(row_name) %>%
      dplyr::summarize(
        Logit.Min = min(Odd.ratio, na.rm = TRUE),
        Logit.Max = max(Odd.ratio, na.rm = TRUE),
        Logit.Mean = mean(Odd.ratio, na.rm = TRUE),
        Logit.Median = median(Odd.ratio, na.rm = TRUE),
        Logit.q0.025 = quantile(Odd.ratio, 0.025, na.rm = TRUE),
        Logit.q0.975 = quantile(Odd.ratio, 0.975, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Add the summaries for the current variable to the result data frame
    summary_results <- rbind(summary_results, grouped_summary)
  }
  
  return(summary_results)
}
```
##7.2 Using random seed to perform the ER analysis
### input the initial dose and parameter dataset
```{r}
setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis")
# mrgsolve model
mod<- mread_cache("KP_2com.mod") 
# Clear the input dose dataset
parameter.451 <- read.csv("estimated individual PK parameters.csv")

dose<- read.csv("dosing record for study 309.csv")
dose.451 <- dose %>%
        mutate(DOSE = as.numeric(DOSE))%>%
        select(-X9)%>%
        mutate(FORM = case_when(
               FORM == "TABLET" ~ 3, 
               FORM == "CAPSULE" ~ 2),
               DOSE = if_else(DOSE == 0, NA, DOSE))%>%
        mutate(DAY = as.numeric(DAY), 
               time = (DAY - 1) * 24,
               C = "") %>%
       dplyr::select(C, SUBJID,time, DAY, DOSE, FORM, SEQ) %>%   
       mutate( evid =1,
               amt=DOSE,
               cmt=1)%>% 
       mutate( SUBJID = as.numeric(SUBJID))
```
There scenarios will be explored:
- EO1: shape = 0.65, scale = 100;
- EO2: shape = 2, scale = 240;
### run the simulation
```{r}

# dose interval could be changed 
DI = 1# DAY
Case.fix <- 1
event.shape <-2
event.scale <- 240
## The 1 to 500 ID has 60&40&20 dose level
FHDOSE <- 100
FMDOSE <- 60
FLDOSE <- 40
## The 501 to 1000 ID has 20&10&5 dose level
HDOSE <- 5
MDOSE <- 2
LDOSE <- 1
# Initialize lists to store the results from each simulation
univ_con_IDQ <- list()
logistic_test <- list()
wilcox_test <- list()

#used for test
# Set a fixed seed for reproducibility
set.seed(2025)
# Randomly select 100 seeds from 1 to 100,000
seeds <- sample(10:10000, 1000)
#loop <- function(seed, df.exp, shape, scale, group_count){
for (seed in seeds) {
  simulation_result <- loop(DI, Case.fix, seed, parameter.451, dose.451, event.shape, event.scale) 
  univ_con_IDQ[[as.character(seed)]] <- simulation_result$result_1
  logistic_test[[as.character(seed)]] <- simulation_result$result_2
  wilcox_test[[as.character(seed)]] <- simulation_result$wilcox_results
  }

setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveRDS(univ_con_IDQ, "univ_con_IDQ.rds")
saveRDS(wilcox_test, "wilcox_test.rds")
saveRDS(logistic_test, "logistic_test.rds")

```
## read saved files if need to get more information
```{r}
univ_con_IDQ <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/univ_con_IDQ.rds")
wilcox_test <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/wilcox_test.rds")
logistic_test <- readRDS("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10/logistic_test.rds")
DI=1
```
##7.3 Abstract P-values
```{r}
# Specify variables for each type of analysis
con_var <- c("Cavg1D", "CavgOED","Cavg1WOED","Cavg2WOED","Cavg4WOED", "CavgTE")
Wilcox_var <- c("Cavg1D", "CavgOED","Cavg1WOED","Cavg2WOED","Cavg4WOED", "CavgTE")

con_IDQ <- process_dataset(univ_con_IDQ, con_var, "con")
Wilcox <- process_dataset(wilcox_test, Wilcox_var, "WTest")
logist <- process_dataset(logistic_test, con_var, "Logist")

tables_list <- c(con_IDQ,logist)
final_table <- bind_rows(tables_list, .id = "Source")

table_0_05 <- final_table %>%
  filter(grepl("Group1", Source)) %>%
  mutate_all(~replace(., is.na(.), 0))


```
##7.4  abstract and summary slope values for logistic regression
```{r}
slope_results <- extract_slope(logistic_test, Slope = "Slope", variables = con_var)
summary_results <- summarize_slopes(slope_results)

logit_results <- lapply(slope_results, function(df) {
  df$Odd.ratio <- exp(df$Slope.Value) # Add the new column
  return(df) # Return the updated data frame
})

setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveRDS(logit_results, "logit results.rds")
```

##7.5  abstract and summary beta values 
```{r}
Beta.cont <- extract_beta_cont(univ_con_IDQ, con_var)
coxPH_odd <- Beta.cont %>% mutate(hazard.ratio = exp(Beta))

Beta.cont_df <- Beta.cont %>%
  group_by(row_name) %>%
  summarise(
    Cox.Min = min(Beta),
    Cox.Max = max(Beta),
    Cox.Mean = mean(Beta),
    Cox.Median = median(Beta),
    Cox.q0.025 = quantile(Beta, 0.025), 
    Cox.q0.975 = quantile(Beta, 0.975)  
    )%>%
   mutate(Beta_variable = "Beta")

Beta.summary <- Beta.cont_df

variable.count<- length(con_var)
Beta.filtered <- Beta.summary[1:variable.count, ]

p.filtered_con <- table_0_05[1:variable.count, ]
p.filtered_logit <- table_0_05[(variable.count+1):(2*variable.count), ]

p_values_con <- p.filtered_con %>%
  select(Source, `p<0.05`) %>%
  mutate(Source = str_replace(Source, "^con_", "")) %>%
  mutate(Source = str_replace(Source, "_Group1$", ""))%>%
  rename(row_name=Source)%>%
  rename(`Cox(P<0.05)` = `p<0.05`)

p_values_logit <- p.filtered_logit %>%
  select(Source, `p<0.05`) %>%
  mutate(Source = str_replace(Source, "^Logist_", "")) %>%
  mutate(Source = str_replace(Source, "_Group1$", ""))%>%
  rename(row_name=Source)%>%
  rename(`Logit(P<0.05)` = `p<0.05`)

loop.summary.logit <-  left_join(p_values_logit, summary_results, by = "row_name")
loop.summary.cox <-  left_join(p_values_con, Beta.filtered, by = "row_name")

loop.summary.logit<- loop.summary.logit %>% rename(Covariate = row_name)
loop.summary.cox<- loop.summary.cox %>% rename(Covariate = row_name) %>%select(-Beta_variable)

wb_summary <- createWorkbook()
addWorksheet(wb_summary, "Logistic Analysis")
addWorksheet(wb_summary, "Univariate CoxPH Analysis")
writeData(wb_summary, sheet = "Logistic Analysis", x = loop.summary.logit)
writeData(wb_summary, sheet = "Univariate CoxPH Analysis", x = loop.summary.cox)

setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveWorkbook(wb_summary, file = "Loop Results_Slope&Beta.xlsx", overwrite = TRUE)
saveRDS(coxPH_odd, "CoxPH results.rds")

logit_odd_results <- logit_odd(logit_results)

Cox_odd_results <- coxPH_odd %>%
  group_by(row_name) %>%
  summarise(
    Cox.Min = min(hazard.ratio),
    Cox.Max = max(hazard.ratio),
    Cox.Mean = mean(hazard.ratio),
    Cox.Median = median(hazard.ratio),
    Cox.q0.025 = quantile(hazard.ratio, 0.025), 
    Cox.q0.975 = quantile(hazard.ratio, 0.975)  
    )

loop.summary.logit <-  left_join(p_values_logit, logit_odd_results, by = "row_name")
loop.summary.cox <-  left_join(p_values_con, Cox_odd_results, by = "row_name")

wb_summary <- createWorkbook()
addWorksheet(wb_summary, "Logistic Analysis")
addWorksheet(wb_summary, "Univariate CoxPH Analysis")
writeData(wb_summary, sheet = "Logistic Analysis", x = loop.summary.logit)
writeData(wb_summary, sheet = "Univariate CoxPH Analysis", x = loop.summary.cox)

setwd("C:/Users/Xuefen.Yin/OneDrive - FDA/ER1 analysis/ER1_DH3_2Dose/ER1_DH3_HL7_EO1_ii24_60&10")
saveWorkbook(wb_summary, file = "Loop Results_odd&hazard_ratio.xlsx", overwrite = TRUE)

```



